<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="application/xml; charset=UTF-8">
      <title>TUTORIAL PREVIEW</title>
      <style type="text/css">
                  .toc { font-size: 80%; padding: 0.4em; outline: thin solid black; margin: 0.4em 0em; width: fit-content;
                    counter-reset: lessonNo 0 }
                  .toc * { margin: 0em; font-weight: normal }
                  .toc div { margin: 0.2em; margin-left: 1em; outline: thin solid grey }
                  .toc .lesson { display: flex; counter-increment: lessonNo 1; padding: 0.8em }
                  .toc .lesson:nth-child(even) { background-color: lightsteelblue }
                  .toc div.lesson:before { content: attr(class) 'Â ' counter(lessonNo); background-color: lavender; color: midnightblue; padding: 0.2em; font-family: sans-serif
                    display: inline-block; height: fit-content }
                  .toc .unit { width: 30vw; background-color: whitesmoke }
                  section section section { margin: 0.2em; margin-left: 1em; padding-left: 0.6em; border-left: medium solid grey }
                  
                  
               </style>
   </head>
   <body>
      <div class="toc">
         <div class="lesson">
            <div class="unit">
               <h1>101: Project setup and installation</h1>
               <div>
                  <h2>Goals</h2>
               </div>
               <div>
                  <h2>Prerequisites</h2>
               </div>
               <div>
                  <h2>Resources</h2>
               </div>
               <div>
                  <h2>Step One: Setup</h2>
                  <div>
                     <h3>Shortcut</h3>
                  </div>
               </div>
               <div>
                  <h2>Step Two: Confirm</h2>
               </div>
               <div>
                  <h2>Comments / review</h2>
                  <div>
                     <h3>Tweaks</h3>
                  </div>
               </div>
            </div>
            <div class="unit">
               <h1>102: Examining the setup</h1>
               <div>
                  <h2>Goals</h2>
               </div>
               <div>
                  <h2>Resources</h2>
               </div>
               <div>
                  <h2>Prerequisites</h2>
               </div>
               <div>
                  <h2>Step One: Inspect the pipelines</h2>
               </div>
               <div>
                  <h2>Step Two: Modify the pipelines</h2>
               </div>
               <div>
                  <h2>For consideration</h2>
               </div>
            </div>
            <div class="unit">
               <h1>599: Meeting XProc</h1>
               <div>
                  <h2>Some observations</h2>
               </div>
            </div>
         </div>
         <div class="lesson">
            <div class="unit">
               <h1>101: Unpacking XProc 3.0</h1>
               <div>
                  <h2>Goals</h2>
               </div>
               <div>
                  <h2>Resources</h2>
               </div>
               <div>
                  <h2>Prerequisites</h2>
               </div>
               <div>
                  <h2>A closer look</h2>
               </div>
               <div>
                  <h2>Messing around</h2>
               </div>
               <div>
                  <h2>XProc coverage index</h2>
               </div>
               <div>
                  <h2>Learning about XProc</h2>
               </div>
            </div>
            <div class="unit">
               <h1>102: XProc fundamentals</h1>
               <div>
                  <h2>Goals</h2>
               </div>
               <div>
                  <h2>Resources</h2>
               </div>
               <div>
                  <h2>Prerequisites</h2>
               </div>
               <div>
                  <h2>Learning about XProc</h2>
               </div>
               <div>
                  <h2>Anatomy of an XProc pipeline</h2>
                  <div>
                     <h3>XProc files and XProc steps</h3>
                     <div>
                        <h4>XProc as an XML document</h4>
                     </div>
                     <div>
                        <h4>XProc embedded documentation</h4>
                     </div>
                  </div>
                  <div>
                     <h3>XProc step prologue and body</h3>
                  </div>
                  <div>
                     <h3>Atomic and compound steps</h3>
                  </div>
                  <div>
                     <h3>Namespaces and extension steps</h3>
                  </div>
                  <div>
                     <h3>Schema for XProc 3.0</h3>
                  </div>
               </div>
               <div>
                  <h2>Take note</h2>
                  <div>
                     <h3>Where are these downloads coming from?</h3>
                  </div>
                  <div>
                     <h3>Syntax tips</h3>
                  </div>
               </div>
               <div>
                  <h2>Exercise: Knowing what to look for</h2>
               </div>
            </div>
            <div class="unit">
               <h1>599: More context</h1>
               <div>
                  <h2>Goals</h2>
               </div>
               <div>
                  <h2>Resources</h2>
               </div>
               <div>
                  <h2>Prerequisites</h2>
               </div>
               <div>
                  <h2>XProc schema</h2>
               </div>
               <div>
                  <h2>XPath</h2>
                  <div>
                     <h3>Documents and data</h3>
                  </div>
               </div>
               <div>
                  <h2>Survey of XProc elements</h2>
               </div>
               <div>
                  <h2>XML time line</h2>
               </div>
               <div>
                  <h2>XPath illustrative examples</h2>
               </div>
               <div>
                  <h2>XML and the XDM: context and rationale</h2>
               </div>
               <div>
                  <h2>Exercise: Discussion board</h2>
               </div>
            </div>
         </div>
         <div class="lesson">
            <div class="unit">
               <h1>101: OSCAL from XML to JSON and back</h1>
               <div>
                  <h2>Goals</h2>
               </div>
               <div>
                  <h2>Prerequisites</h2>
               </div>
               <div>
                  <h2>Resources</h2>
               </div>
               <div>
                  <h2>Step zero: an identity pipeline</h2>
               </div>
               <div>
                  <h2>Step one: convert some OSCAL XML into OSCAL JSON</h2>
                  <div>
                     <h3>The playing field is the internet</h3>
                  </div>
               </div>
               <div>
                  <h2>Step two: return trip</h2>
               </div>
               <div>
                  <h2>What is this XSLT?</h2>
               </div>
               <div>
                  <h2>What could possibly go wrong?</h2>
                  <div>
                     <h3>Intercepting errors</h3>
                  </div>
               </div>
            </div>
            <div class="unit">
               <h1>102: Hands on data conversions</h1>
               <div>
                  <h2>Goals</h2>
               </div>
               <div>
                  <h2>Prerequisites</h2>
               </div>
               <div>
                  <h2>Resources</h2>
               </div>
               <div>
                  <h2>Probing error space - data conversions</h2>
                  <div>
                     <h3>Converting broken XML or JSON</h3>
                  </div>
                  <div>
                     <h3>Converting broken OSCAL</h3>
                  </div>
                  <div>
                     <h3>Converting not-OSCAL</h3>
                  </div>
               </div>
               <div>
                  <h2>XProc diagnostic how-to</h2>
                  <div>
                     <h3>Emitting runtime messages</h3>
                  </div>
                  <div>
                     <h3>Saving out interim results</h3>
                  </div>
               </div>
               <div>
                  <h2>Validate early and often</h2>
               </div>
               <div>
                  <h2>for 599: XProc for JSON</h2>
               </div>
               <div>
                  <h2>for 599: YAML TODO</h2>
               </div>
               <div>
                  <h2>for 599: XProc port bindings</h2>
               </div>
               <div>
                  <h2>for 599: URIs and URI schemes</h2>
               </div>
               <div>
                  <h2>for 599: round tripping as process test</h2>
               </div>
            </div>
         </div>
         <div class="lesson">
            <div class="unit">
               <h1>101: Seeing valid OSCAL</h1>
               <div>
                  <h2>Goals</h2>
               </div>
               <div>
                  <h2>Prerequisites</h2>
               </div>
               <div>
                  <h2>Resources</h2>
               </div>
               <div>
                  <h2>Step one: validate some OSCAL XML</h2>
                  <div>
                     <h3>Boldly coding</h3>
                  </div>
               </div>
               <div>
                  <h2>Step two: validate and report</h2>
                  <div>
                     <h3>Optional: confirm the validation</h3>
                  </div>
               </div>
               <div>
                  <h2>Step three: Run a batch validator</h2>
                  <img src="ugly-traceback.png"
                       alt="ugly-traceback.png">
               </div>
               <div>
                  <h2>What could possibly go wrong?</h2>
                  <div>
                     <h3>When you know your schema</h3>
                  </div>
                  <div>
                     <h3>When you must determine a schema</h3>
                  </div>
               </div>
               <div>
                  <h2>What is this schema?</h2>
                  <div>
                     <h3>Schemas and document types</h3>
                  </div>
               </div>
               <div>
                  <h2>Critique so far</h2>
               </div>
            </div>
            <div class="unit">
               <h1>102: Validating OSCAL</h1>
               <div>
                  <h2>Goals</h2>
               </div>
               <div>
                  <h2>Prerequisites</h2>
               </div>
               <div>
                  <h2>Resources</h2>
               </div>
               <div>
                  <h2>Schema validation under XProc / code review</h2>
               </div>
               <div>
                  <h2>validating sets of documents, handling outputs</h2>
                  <div>
                     <h3>Rule of thumb - <code>assert-valid</code>
                     </h3>
                  </div>
               </div>
               <div>
                  <h2>Review: why validate?</h2>
               </div>
               <div>
                  <h2>102: Defining a pipeline step</h2>
               </div>
               <div>
                  <h2>102: Other schemas vs/and other models</h2>
               </div>
               <div>
                  <h2>102: XProc exception handling</h2>
               </div>
               <div>
                  <h2>599: some XProc features</h2>
               </div>
            </div>
         </div>
         <div class="lesson">
            <div class="unit">
               <h1>101: Some OSCAL publishing</h1>
               <div>
                  <h2>Goals</h2>
               </div>
               <div>
                  <h2>Prerequisites</h2>
               </div>
               <div>
                  <h2>Resources</h2>
               </div>
               <div>
                  <h2>Step one: produce HTML</h2>
               </div>
               <div>
                  <h2>Step two: edit and adjust</h2>
               </div>
            </div>
            <div class="unit">
               <h1>102: More OSCAL Publishing</h1>
               <div>
                  <h2>Goals</h2>
               </div>
               <div>
                  <h2>Prerequisites</h2>
               </div>
               <div>
                  <h2>Resources</h2>
               </div>
               <div>
                  <h2>Step One: any old OSCAL catalog</h2>
               </div>
               <div>
                  <h2>Step Two: publishing OSCAL JSON</h2>
               </div>
            </div>
         </div>
         <div class="lesson">
            <div class="unit">
               <h1>101: Producing OSCAL from a publication format</h1>
               <div>
                  <h2>Goals</h2>
               </div>
               <div>
                  <h2>Prerequisites</h2>
               </div>
               <div>
                  <h2>Resources</h2>
               </div>
               <div>
                  <h2>Step one: acquire resources</h2>
               </div>
               <div>
                  <h2>Step two: examine HTML source data</h2>
                  <div>
                     <h3>Warning: fragile!</h3>
                  </div>
               </div>
               <div>
                  <h2>Step three: run the pipeline</h2>
               </div>
               <div>
                  <h2>Step four (optional): run the pipeline in diagnostic mode</h2>
               </div>
            </div>
            <div class="unit">
               <h1>102: Producing OSCAL from uncooked data</h1>
               <div>
                  <h2>Goals</h2>
               </div>
               <div>
                  <h2>Prerequisites</h2>
               </div>
               <div>
                  <h2>Resources</h2>
               </div>
               <div>
                  <h2>Step one: run the pipeline in diagnostic mode</h2>
               </div>
               <div>
                  <h2>Step two: survey the pipeline steps</h2>
                  <div>
                     <h3>feature: step by step <q>up the hill</q>
                     </h3>
                  </div>
                  <div>
                     <h3>feature: saving intermediate files conditionally</h3>
                  </div>
                  <div>
                     <h3>feature: inline and out-of-line XSLT transformations</h3>
                  </div>
                  <div>
                     <h3>feature: validations on the fly</h3>
                  </div>
               </div>
               <div>
                  <h2>Step 2.5: Inspect the STS version</h2>
               </div>
               <div>
                  <h2>Step three: break and repair</h2>
               </div>
               <div>
                  <h2>Step four: research XSLT</h2>
               </div>
            </div>
         </div>
      </div>
      <section class="lesson"
               id="setup">
         <section class="unit"
                  id="setup_101">
            <h1>101: Project setup and installation</h1>
            <section>
               <h2>Goals</h2>
               <p>Run an XProc 3.0 pipeline in an XProc 3.0 engine. See the results.</p>
               <p>With a little practice, become comfortable running XProc pipelines, seeing results on a console (command
            line) window as well as in the file system.</p>
               <p>After the first script to get the XProc engine, we use XProc for subsequent downloads. Finishing the setup
            gets you started practicing with the pipelines.</p>
            </section>
            <section>
               <h2>Prerequisites</h2>
               <p>You have Java installed with a JVM (Java Virtual Machine) available on the command line (a JRE or JDK),
            version 8 (and later).</p>
               <p>You have a live Internet connection and the capability to download and save resources (binaries and code
            libraries) for local use.</p>
               <p>
                  <b>Tip:</b> check your Java version from the console using <code>java --version</code>.</p>
               <p>You are comfortable entering commands on the command line. For installation, you want a <code>bash</code>
            shell if available. On Windows, both WSL (Ubuntu) and Git Bash have been found to work. If you cannot use
               <code>bash</code>, the setup can be done by hand (downloading and unpacking a package from
            SourceForge).</p>
               <p>After installation, subsequent work on Windows does not require <code>bash</code> unless you choose to use
            it â a Windows <code>CMD</code> or Powershell can serve as your environment and the processor invoked with a
            Windows <code>bat</code> file (as described in the documentation). Mac and Linux (and WSL) users can
            continue to use <code>bash</code>.</p>
            </section>
            <section>
               <h2>Resources</h2>
               <p>The setup script is a <code>bash</code> script: <a href="../../../setup.sh">./setup.sh</a>, to be run with no arguments.</p>
               <p>For XProc runtime â to execute pipelines â use either of the scripts <a href="../../../xp3.sh">./xp3.sh</a> (under <code>bash</code>) or <a href="../../../xp3.bat">./xp3.bat</a> (for Windows). These scripts are
            used for all pipelines (basically, for everything) unless otherwise noted.</p>
               <p>The following pipelines will be run. They are described in top-level <a href="../../../README.md">README</a> documentation and the expected places.</p>
               <ul>
                  <li>
                     <a href="../../../lib/GRAB-SAXON.xpl">lib/GRAB-SAXON.xpl</a>
                  </li>
                  <li>
                     <a href="../../../lib/GRAB-SCHXSLT.xpl">lib/GRAB-SCHXSLT.xpl</a>
                  </li>
                  <li>
                     <a href="../../../lib/GRAB-XSPEC.xpl">lib/GRAB-XSPEC.xpl</a>
                  </li>
                  <li>
                     <a href="../../../smoketest/TEST-XPROC3.xpl">smoketest/TEST-XPROC3.xpl</a>
                  </li>
                  <li>
                     <a href="../../../smoketest/TEST-XSLT.xpl">smoketest/TEST-XSLT.xpl</a>
                  </li>
                  <li>
                     <a href="../../../smoketest/TEST-SCHEMATRON.xpl">smoketest/TEST-SCHEMATRON.xpl</a>
                  </li>
                  <li>
                     <a href="../../../smoketest/TEST-XSPEC.xpl">smoketest/TEST-XSPEC.xpl</a>
                  </li>
               </ul>
            </section>
            <section>
               <h2>Step One: Setup</h2>
               <p>Find setup instructions for the repository in the <a href="../../../README.md">Project README</a> and in the linked <a href="../../../setup-notes.md">Setup Notes</a>.</p>
               <p>After reading and reviewing these documents, perform the setup as instructed. To do this you can either
            fork or clone the repository in GitHub or simply download and decompress a zip of the <a href="https://github.com/usnistgov/oscal-xproc3/archive/refs/heads/main.zip">current
            distribution</a>.</p>
               <p>After running the setup script, or performing the installation by hand, make sure you can run all the smoke
            tests successfully.</p>
               <p>As noted in the docs, if you happen already to have <a href="https://www.xml-project.com/morganaxproc-iiise.html">Morgana XProc III</a>, you do not need to
            download it again. Try skipping straight to the smoke tests. (You can use a runtime script
               <code>xp3.sh</code> or <code>xp3.bat</code> as a model for your own, and adjust. Any reasonably recent
            version of Morgana should function if configured correctly, and we are interested if it does not.) </p>
               <section>
                  <h3>Shortcut</h3>
                  <p>If you want to run through the tutorial exercises but you are unsure of how deeply you will delve, you
               can postpone two of the installations until later:</p>
                  <ul>
                     <li>You will need XSpec only when you want to run tests of stylesheets or queries using the <a href="https://github.com/xspec/xspec">XSpec</a> testing framework</li>
                     <li>You will need SchXSLT only when you want to run Schematron (or XSpec tests of Schematron)</li>
                  </ul>
                  <p>When you see tracebacks suggesting one of these is not supported, you can return to setup.</p>
                  <p>Since almost any pipeline will use XSLT and since we do use the latest version (XSLT 3.0 with XPath 3.1), consider
               the Saxon installation an essential requirement.</p>
               </section>
            </section>
            <section>
               <h2>Step Two: Confirm</h2>
               <p>The top-level README and setup notes also describe testing your installation. Do this next.</p>
               <p>You know things are working in your XProc when either or both of two things are happening:</p>
               <ul>
                  <li>On the console, notifications show up with reassuring messages announcing progress</li>
                  <li>When you expect files to be produced, they appear, or are updated, as expected</li>
               </ul>
               <p>Both of those will occur with this lesson. The files produced by downloading scripts are written into the
            project <code>lib</code> directory, as documented. Refresh or restore by deleting the downloaded files and
            running the pipelines to acquire them again.</p>
               <p>Note: you need a live Internet connection for your http requests to go through.</p>
               <p>When you can run all the smoke tests without ugly tracebacks, this lesson is complete.</p>
            </section>
            <section>
               <h2>Comments / review</h2>
               <p>Within this project as a whole, and within its subprojects, everything is done with XProc 3.0, meaning
            everything can be done using a single script, which invokes an XProc processor to read and execute a
            pipeline. This simplicity is at the center of the argument for XProc. The counter argument is that the world
            is never like that. This project is based on the premise that a lab environment can be useful even for
            studying things more commonly encountered in the real world.</p>
               <p>Effectively (and much more could be said about the processing stack, dependency management and so forth)
            what this means is that XProc offers the user and the developer (in either or both roles) with focused and
            concentrated points of control or points of adjustment. In the field -- where software is deployed and
            used -- things almost never just <q>drop in</q>. User interfaces, APIs, dependencies and platform quirks: all these 
            constrain what users can do, and even developers are rarely free to just mess around, as it might be thought of.</p>
               <p>To the extent this is the case, this project only works if things are actually simple enough to pick up,
            use, learn and adapt.</p>
               <p>
                  <code>xp3.sh</code> and <code>xp3.bat</code> represent attempts at this. Each of them (on its execution
            platform) enables a user to run, without further configuration, the <a href="https://www.xml-project.com/morganaxproc-iiise.html">Morgana XProcIIIse</a> processor on any XProc
            3.0 pipeline, assuming the appropriate platform for each (<code>bash</code> in the case of the shell script,
            Windows batch command syntax for the <code>bat</code> file). Other platforms supporting Java (and hence
            Morgana with its libraries) could be provided with similar scripts.</p>
               <p>Such a script itself must be <q>vanilla</q> and generic: it simply invokes the processor with the designated
            pipeline, and stands back. The logic of operations is entirely encapsulated in the XProc pipeline
            designated. XProc 3.0 is both scalable and flexible enough to open a wide range of possibilities for data
            processing, both XML-based and using other formats such as JSON and plain text. It is the intent of this
            project not to explore and map this space â which is vast â but to show off enough XProc and related logic
            (XSLT, XSpec) to show how this exploration can be done. We are an outfitter at the beginning of what
            we hope will be a long and profitable voyage.</p>
               <section>
                  <h3>Tweaks</h3>
                  <p>As simple examples, these scripts show only one way of running XProc. Keep in mind that even simple
               scripts can be used in more than one way. </p>
                  <p>For example, a pipeline can be executed from the project root:</p>
                  <pre>$ ./xp3.sh smoketest/TEST-XPROC3.xpl</pre>
                  <p>Alternatively, a pipeline can be executed from its home directory, for example if currently in the
                  <code>smoketest</code> directory (note the path to the script): </p>
                  <pre>$ ../xp3.sh TEST-XPROC3.xpl</pre>
                  <p>This works the same ways on Windows, with adjustments: </p>
                  <pre>&gt; ..\xp3 TEST-XPROC3.xpl </pre>
                  <p>(On Windows a <code>bat</code> file suffix marks it as executable and does not have to be given
               explicitly when calling.)</p>
                  <p>Windows users (and others to varying degrees) can set up a drag-and-drop based workflow - using your
               mouse or pointer, select an XProc pipeline file and drag it to a shortcut for the executable (Windows
               batch file). A command window opens to show the operation of the pipeline. See the <a href="../../README.md">README</a> for more information.</p>
                  <p>It is important to try things out since any of these methods can be the basis of a workflow. </p>
                  <p>For the big picture, keep in mind that while the command line is useful for development and demonstration
               â and however familiar XProc itself may become to the developer â to the uninitiated it remains obscure
               and cryptic. XProc-based systems, when integrated into tools or developer editors and environments, can
               look much nicer than tracebacks in a console window. The beauty we are looking for here is in a different
               kind of elegance and power.</p>
               </section>
            </section>
         </section>
         <section class="unit"
                  id="setup_102">
            <h1>102: Examining the setup</h1>
            <section>
               <h2>Goals</h2>
               <ul>
                  <li>Look at some pipeline organization and syntax on the inside</li>
                  <li>Success and failure invoking XProc pipelines: an early chance to "learn to die" gracefully (to use the
               gamers' idiom).</li>
               </ul>
            </section>
            <section>
               <h2>Resources</h2>
               <p>Same as <a href="setup_101_src.html"
                     class="LessonUnit">Setup 101</a>.</p>
            </section>
            <section>
               <h2>Prerequisites</h2>
               <p>Please complete the repository setup and smoke tests as described in the <a href="setup_101_src.html"
                     class="LessonUnit">101 lesson</a>. In this lesson, we will run these pipelines with adjustments, or
            similar pipelines.</p>
               <p>This discussion assumes basic knowledge of coding, the Internet (including retrieving resources via
               <code>file</code> and <code>http</code> protocols), and web-based technologies including HTML.</p>
               <p>XML knowledge is also assumed. In particular, XProc uses <a href="https://www.w3.org/TR/xpath-31/">XPath
               3.1</a>, the query language for XML. This latest version of XPath builds on XPath 1.0, so any XPath
            experience will help. In general, any XSLT or XQuery experience will be invaluable.</p>
               <p>You will also need a programmer's plain text editor, XML/XSLT editor or IDE (integrated development
            environment) for more interactive testing of the code.</p>
            </section>
            <section>
               <h2>Step One: Inspect the pipelines</h2>
               <p>The two groupings of pipelines used in setup and testing can be considered separately.</p>
               <p>The key to understanding both groups is to know that once the initial <a href="../../../setup.sh">Setup
               script</a> is run, Morgana can be invoked directly, as paths and scripts are already in place. In doing
            so â before extension libraries are in place â it can use only basic XProc steps, but those are enough for
            these purposes.</p>
               <p>Specifically, the pipelines can acquire resources from the Internet, save them locally, and perform
            unarchiving (unzipping). Having been downloaded, each library provides software that the pipeline engine
            (Morgana) can use to do more.</p>
               <p>Accordingly, the first group of pipelines (in the <a href="../../../lib/readme.md">lib</a> directory has
            a single purpose, namely (together and separately) to download software for Morgana.</p>
               <ul>
                  <li>
                     <a href="../../../lib/GRAB-SAXON.xpl">lib/GRAB-SAXON.xpl</a>
                  </li>
                  <li>
                     <a href="../../../lib/GRAB-SCHXSLT.xpl">lib/GRAB-SCHXSLT.xpl</a>
                  </li>
                  <li>
                     <a href="../../../lib/GRAB-XSPEC.xpl">lib/GRAB-XSPEC.xpl</a>
                  </li>
               </ul>
               <p>The second group of pipelines also has a single purpose, namely to exercise and test the capabilities provided by the software downloaded by the first group.</p>
               <ul>
                  <li>
                     <a href="../../../smoketest/TEST-XPROC3.xpl">smoketest/TEST-XPROC3.xpl</a> tests MorganaXProc-III</li>
                  <li>
                     <a href="../../../smoketest/TEST-XSLT.xpl">smoketest/TEST-XSLT.xpl</a> tests Saxon</li>
                  <li>
                     <a href="../../../smoketest/TEST-SCHEMATRON.xpl">smoketest/TEST-SCHEMATRON.xpl</a> tests
               SchXSLT</li>
                  <li>
                     <a href="../../../smoketest/TEST-XSPEC.xpl">smoketest/TEST-XSPEC.xpl</a> tests XSpec</li>
               </ul>
               <p>Take a look at these. If it is helpful, try to see the XML syntax as a set of nested frames with labels and
            connectors.</p>
               <p>Try more than one way of looking at the XProc source code: in the Github repository, on your file system, in
            a plain text editor, in an XML editor.</p>
            </section>
            <section>
               <h2>Step Two: Modify the pipelines</h2>
               <p>Use a text editor or IDE for this exercise.</p>
               <p>If you have any concepts for improvements to the pipelines, or other resources that might be acquired this way, copy and modify one of the pipelines given to achieve those results.</p>
               <p>Even if not - be sure to break the pipelines given -- or copies under new names -- in any of several ways.
            Then run the modified pipelines, as a <i>safe way</i> to familiarize yourself with error messages:</p>
               <ul>
                  <li>Break the XML syntax of a pipeline and try to run it</li>
                  <li>Leave XML syntax intact (well-formed), but break something in the XProc <ul>
                        <li>An element name, attribute or attribute setting</li>
                        <li>A namespace</li>
                     </ul>
                  </li>
                  <li>Try to retrieve something from a broken link</li>
               </ul>
               <p>Make sure that pipelines are back in working order when this exercise is complete.</p>
            </section>
            <section>
               <h2>For consideration</h2>
               <p>Developers coming to this technology need to consider who would use it, and whether it is useful mainly at
            the back end, or also <q>on the shop floor</q>, directly in the hands of professionals who must work with
            the data, bringing expertise in subject matter (such as, for OSCAL, systems security documentation) but not
            in data processing as such.</p>
               <p>Key to this question is not only whether attractive and capable user interfaces (or other mediators) can be
            developed (this is a known problem) but more importantly whether the systems themselves are adaptable enough
            so they can be deployed, used, refitted and maintained not just for repetitive generic tasks, but for
               <i>particular</i>, <i>special</i> and <i>local</i> problems discovered only at the points where
            information is gathered and codified.</p>
               <p>This larger fitting of solutions to problems is a responsibility for both SMEs (subject matter experts) and
            developers together, who must define problems to be solved before approaches to them can be found.</p>
               <p>The open questions are: who can use XProc pipelines; and how can they be made more useful? The questions
            come up in an OSCAL context or any context where XML is demonstrably capable.</p>
            </section>
         </section>
         <section class="unit"
                  id="setup_599">
            <h1>599: Meeting XProc</h1>
            <section>
               <h2>Some observations</h2>
               <p>Because it is now centered on <i>pipelines</i> as much as on files and software packages, dependency
            management is different from other technologies including Java and NodeJS - how so?</p>
               <p>MorganaXProc-III is implemented in Scala, and Saxon is built in Java, but otherwise distributions including
            the SchXSLT and XSpec distributions consist mainly of XSLT. This is either very good (with development and
            maintenance requirements in view), or not good at all.</p>
               <p>How much of this is due to the high-level, abstracted nature of <a href="https://en.wikipedia.org/wiki/Fourth-generation_programming_language">4GLs</a> including both XSLT
            3.1 and XProc 3.0?</p>
            </section>
         </section>
      </section>
      <section class="lesson"
               id="unpack">
         <section class="unit"
                  id="unpack_101">
            <h1>101: Unpacking XProc 3.0</h1>
            <p>
            </p>
            <section>
               <h2>Goals</h2>
               <ul>
                  <li>More familiarity with XProc 3.0: some syntax</li>
                  <li>History, concepts and resources</li>
               </ul>
            </section>
            <section>
               <h2>Resources</h2>
               <p>The same pipelines you ran in setup: <a href="../setup/setup_101_src.html"
                     class="LessonUnit">Setup 101</a>.</p>
               <p>Also, <a href="https://xproc.org">XProc.org dashboard page</a>
               </p>
               <p>Also, XProc index materials produced in this repository: <a href="../../../projects/xproc-doc/readme.md">XProc
               docs</a>
               </p>
            </section>
            <section>
               <h2>Prerequisites</h2>
               <p>Same as <a href="../setup/setup_101_src.html"
                     class="LessonUnit">Setup 101</a>.</p>
            </section>
            <section>
               <h2>A closer look</h2>
               <p>If you have completed <a href="../setup/setup_101_src.html"
                     class="LessonUnit">Setup 102</a> you have
            already glanced at the <a href="../../../lib/readme.md">lib</a> and <a href="../../../smoketest/readme.md">smoketest</a> folders, and run pipelines you have found therein. To edit these files, use any
            XML-capable plain text editor (that is, with care, any editor at all that save text files as UTF-8).</p>
               <p>Routine code inspection can also be done on Github as well (not a bad idea in any case), not just in a
            copy.</p>
               <p>A quick summary of what these pipelines do:</p>
               <ul>
                  <li>
                     <a href="../../../lib/GRAB-SAXON.xpl">lib/GRAB-SAXON.xpl</a> downloads a zip file from a <a href="https://www.saxonica.com/download">Saxonica download site</a>, saves it, and extracts a
                  <code>jar</code> (Java library) file, which it places in the Morgana library directory</li>
                  <li>
                     <a href="../../../lib/GRAB-SCHXSLT.xpl">lib/GRAB-SCHXSLT.xpl</a> downloads a zip file from Github and
               unzips it into a directory where Morgana can find it.</li>
                  <li>
                     <a href="../../../lib/GRAB-XSPEC.xpl">lib/GRAB-XSPEC.xpl</a> also downloads and <q>unarchives</q> a
               zip file resource, this time a copy of <a href="https://github.com/xspec/xspec">an XSpec
               distribution</a>.</li>
               </ul>
               <p>Essentially, these all replicate and capture the work a developer must do to identify and acquire libraries.
            Maintaining our dependencies this way - not quite, but almost <q>by hand</q>, -- appears to have benefits
            for system transparency and robustness.</p>
               <p>The second group of pipelines is a bit more interesting. Each of the utilities provided for in packages just
            downloaded is tested by running a smoke test.</p>
               <p>Each smoke test performs a minor task, serving as a <q>smoke test</q> inasmuch as its only aim is to
            determine whether a simple representative process completes successfully. (When we plug in the board, can we
            see and smell smoke?)</p>
               <ul>
                  <li>
                     <a href="../../../smoketest/TEST-XPROC3.xpl">smoketest/TEST-XPROC3.xpl</a> amounts to an XProc <q>Hello
                  World</q>. In that spirit, feel free to edit and adjust this file.</li>
                  <li>
                     <a href="../../../smoketest/TEST-XSLT.xpl">smoketest/TEST-XSLT.xpl</a> tests Saxon, an
               XSLT/XQuery transformation engine. XSLT and XQuery are related technologies (different languages, same
               data model) developed with XML processing in mind, but in recent years generalized to a wider range of
               data structures.</li>
                  <li>
                     <a href="../../../smoketest/TEST-SCHEMATRON.xpl">smoketest/TEST-SCHEMATRON.xpl</a> tests
               SchXSLT. SchXSLT is an implementation of Schematron, an ISO-standard validation and reporting technology.
               Schematron relies on XSLT, so this library requires Saxon.</li>
                  <li>
                     <a href="../../../smoketest/TEST-XSPEC.xpl">smoketest/TEST-XSPEC.xpl</a> tests XSpec, an
               XSLT-based testing framework useful for testing deployments of XSLT, XQuery and Schematron.</li>
               </ul>
               <p>All these together comprise more than can be learned in an afternoon, or even a week. Any and each of them
            can nonetheless be used as a <q>black box</q> by any competent operator, even without understanding about
            internals.</p>
               <p>At the same time, common foundations make it possible to learn these technologies together and in
            tandem.</p>
               <p>In particular, they all share a syntax for interrogating (querying) the structure of an XML document and
            returning its data: the XPath expression language.</p>
            </section>
            <section>
               <h2>Messing around</h2>
               <p>Taking some time to make and test small adjustments to working code is a great way to develop a sense of how
            it behaves.</p>
               <p>An easy way to do this without perturbing the working code in the repository is to copy a pipeline and
            modify the copy. Modifying one of the smoketest pipelines, see what happens when:</p>
               <ul>
                  <li>An <code>href</code> points to a location on the system where there is no file</li>
                  <li>A file is there, but it is not what is expected (for example: XML is expected but the file is not well
               formed)</li>
                  <li>Individual steps are excluded</li>
                  <li>New elements are renamed (etc.)</li>
               </ul>
               <p>When changes introduce errors, runtime failures and tracebacks will <i>sometimes</i> appear. The indicated
            problem or the source of the reported problem must be repaired. And sometimes a process will run
            successfully. Whether it is in error then depends on how well it conforms to its requirements. Does it
            deliver the results we want and expect?</p>
               <p>As an exercise, make some changes in copies of the test pipelines. Make at least one change that produces
            outputs (such as echoing a document to the console) that are visibly different from the results of the
            original pipeline.</p>
            </section>
            <section>
               <h2>XProc coverage index</h2>
               <p>While they are sometimes rudimentary, the pipelines in this repository address realistic requirements, and
            as such, offer examples of XProc usage.</p>
               <p>A pipeline can process the XProc files in the projects and report where and how they use XProc. <a href="../../PRODUCE-TUTORIAL-ELEMENTLIST.xpl">An available prototype</a> does this in two forms:</p>
               <ul>
                  <li>Assuming the projects are sequenced with the lessons that discuss them, the pipeline gives a listing of
                  <i>first appearances</i> of XProc elements among the examples</li>
                  <li>With this, it gives a comprehensive index of XProc elements by name, appearing in the projects covered
               by the lessons</li>
               </ul>
               <p>The Markdown results are written to the file <a href="../../sequence/element-directory.md">
                  </a>. It can be
            refreshed by running <a href="../../PRODUCE-TUTORIAL-ELEMENTLIST.xpl">the pipeline</a>again.</p>
            </section>
            <section>
               <h2>Learning about XProc</h2>
               <p>This tutorial has a handmade<a href="../../xproc-dashboard.md">XProc dashboard page</a> with links.</p>
               <p>Also, see the official <a href="https://xproc.org">XProc.org dashboard page</a>.</p>
               <p>Also, check out XProc index materials (with code snips) produced in this repository: <a href="../../../projects/xproc-doc/readme.md">XProc docs</a>. Produced using XProc, these
            can be covered in detail in a later lesson unit.</p>
               <p>There is <a href="https://xmlpress.net/publications/xproc-3-0/">a book, Erik Siegel's <i>XProc 3.0
                  Programmer's Reference</i>
                  </a> (2020).</p>
            </section>
         </section>
         <section class="unit"
                  id="unpack_102">
            <h1>102: XProc fundamentals</h1>
            <p>
            </p>
            <section>
               <h2>Goals</h2>
               <ul>
                  <li>More familiarity with XProc 3.0: more syntax</li>
                  <li>History, concepts and resources</li>
               </ul>
            </section>
            <section>
               <h2>Resources</h2>
               <p>Take a quick look <i>now</i>:</p>
               <p>This tutorial's handmade <a href="../../xproc-dashboard.md">XProc dashboard</a> with links</p>
               <p>Also, the official <a href="https://xproc.org">XProc.org dashboard page</a>
               </p>
               <p>Also, check out XProc index materials produced in this repository: <a href="../../../projects/xproc-doc/readme.md">XProc docs</a>
               </p>
               <p>And the same pipelines you ran in setup: <a href="../setup/setup_101_src.html"
                     class="LessonUnit">Setup 101</a>.</p>
            </section>
            <section>
               <h2>Prerequisites</h2>
               <p>You have done <a href="../setup/setup_101_src.html"
                     class="LessonUnit">Setup 101</a>, <a href="../setup/setup_101_src.html"
                     class="LessonUnit">Setup 102</a> and <a href="unpack_101_src.html"
                     class="LessonUnit">Unpack 101</a>.</p>
            </section>
            <section>
               <h2>Learning about XProc</h2>
               <ul>
                  <li>Search engines: use keywords <q>XProc3</q> or <q>XProc 3.0</q> to help distinguish from 1.0 technologies</li>
                  <li>Resources: links here and elsewhere</li>
                  <li>Hands on exercises</li>
                  <li>Work the notes - save out and annotate these pages</li>
               </ul>
            </section>
            <section>
               <h2>Anatomy of an XProc pipeline</h2>
               <section>
                  <h3>XProc files and XProc steps</h3>
                  <p>The <i>step</i> is the core conceptual unit of XProc. An XProc processing pipeline is composed of steps.
               But a pipeline is also considered as a step in itself. As such it can be used in other pipelines, and so
               on.</p>
                  <p>In other words, steps in XProc are <i>compositional</i>. They are building block assemblies made out of
               smaller building block assemblies. A step is a way to process data. A pipeline is a way of orchestrating
               and arranging such processes.</p>
                  <p>The distinction between pipelines and steps is relative and provisional, but important and useful. The
               pipeline is the logical and actual definition of how your data is to be processed. Every pipeline is
               composed of an arrangement, often a series, of operations. The definitions - the steps - include
                  <q>primitives</q>, being designed for generality and reusability for the most common operations. But
               since a pipeline is also a step, we can always combine more than the primitives or core compound steps of
               the language. At a higher level, defining new steps with new step declarations, and using them in
               combination with other steps, is how we manage complexity and change in the requirements. This strategy
               can support for adaptability while also supporting an 'incremental maturity model', improving with reuse,
               building and testing over time. Careful use and deployment of new steps is how we save work, by focusing
               optimization and making it possible to scale up to address data processing requirement sets that are both
               large and complex.</p>
                  <p>Accommodating this design, an XProc <i>file</i> considered as an XML instance is either of two things: a
                  <i>step declaration</i>, or a collection of such declarations, a <i>library</i>.</p>
                  <p>Additionally, step declarations can include their own pipeline (step) declarations, making a hybrid
               architecture: the pipeline comprises a step, with its own library not imported but in line. This can be
               useful.</p>
                  <p>An example of a step library in this repository is <a href="../../../xspec/xspec-execute.xpl">xspec-execute.xpl</a>, which collects several steps supporting XSpec, one each for supporting the
               XSpec testing framework for XSLT, XQuery and Schematron respectively.</p>
                  <p>The advantage of defining a step at the top level, rather than putting all steps into libraries, is that
               such a step can be invoked without prior knowledge of its type name, which is used by XProc to
               distinguish it from other steps. </p>
                  <p>At the top level, recognize an XProc step declaration by the element, <code>p:declare-step</code> (in the
               XProc namespace) and a library by the element <code>p:library</code>.</p>
                  <pre>&lt;p:declare-step xmlns:p="http://www.w3.org/ns/xproc" version="3.0" 
    name="a-first-step"&gt;
...
&lt;/p:declare-step&gt;</pre>
                  <section>
                     <h4>XProc as an XML document</h4>
                     <p>Like any language using XML syntax, XProc depends on a conceptual relation between primitive
                  constructs of the language, and XML syntax, a relation that is ordinarily (and usefully) mediated by
                  means of an (actual or putative) XML <i>data model</i> including elements, attributes, comment nodes,
                  text nodes and so forth. XSLT is such a language, for example: it has its top-level
                     <i>declarations</i>, its <i>template rules</i> and its <i>instructions</i>, all of which are
                  represented using elements in the (standard and most commonly used) XML syntax. Part of learning XSLT
                  is learning that <code>xsl:key</code> is a declaration while <code>xsl:template</code> is a template
                  rule.</p>
                     <p>In the same way, elements in XProc's XML vocabulary correspond to structures in XProc - structures
                  which developers and users rely on, as they define both the internals and the <q>control interface</q>
                  for the language as a semantic construct - something that <q>does something</q>. In XProc, those
                  structures include things like <b>documents</b>, <b>content-types</b> (think of <q>formats</q> such as
                  XML and JSON), <b>ports</b> and <b>steps</b>. Some XProc elements represent steps, others do not. (In
                  the same way as an XSLT key declaration is not a template rule.) Learning this difference among others
                  is how you learn XProc.</p>
                     <p>Fortunately, the vocabulary of the language is not very large. Core XProc has only 95 elements defined
                  in its namespace (or 99, if you are strictly counting all element types defined, not just the names
                  those elements are given). This includes elements for all the core and community-defined steps
                  (recognizable by the prefix <code>p:</code>). Additional to these 95 might be other steps you acquire
                  or define. As with any lanuage, there are parts you will hardly ever use, while other parts are used
                  routinely.</p>
                  </section>
                  <section>
                     <h4>XProc embedded documentation</h4>
                     <p>An example of this is the XProc <code>p:documentation</code> element. This element is designed to
                  carry documentation to a consuming application. Rather than mandate some kind of behavior for
                     <code>p:documentation</code> â something difficult or impossible to do for the general case, or to
                  test â- the XProc rule is <q>anything marked as documentation is for some other consumer</q>, i.e. a
                  documentation engine, not the XProc processor. In other words, a conformant processor <a href="https://spec.xproc.org/3.0/xproc/#documentation">
                           <i>must ignore</i> anything it sees</a>
                  inside <code>p:documentation</code>.</p>
                     <p>There is a small loophole, namely that the effect of <code>p:inline</code> for capturing XML overrides
                  this provision, so if you put <code>p:documentation</code> inside <code>p:inline</code>, it <q>becomes
                     visible</q> - as inline content, not as XProc to be operated on.</p>
                  </section>
               </section>
               <section>
                  <h3>XProc step prologue and body</h3>
                  <p>Keep in mind that every XProc pipeline is also, potentially and actually, a step. There are two things we
               need to know about steps - how to define them, and how to use them.</p>
                  <p>We begin with how to recognize and use steps. But because an XProc pipeline is also an XProc step, we
               can't use steps without ending up with a pipeline. We have only to look at the working pipeline we make
               with our steps, to see how a step is made.</p>
                  <p>As described in the <a href="https://spec.xproc.org/3.0/xproc/#declare-pipelines">XProc 3.0
                  specification</a>, XProc step declarations can be divided into an initial set of elements for setup
               and configuration, followed by what the specification calls a <i>subpipeline</i>, consisting of a
               sequence of steps to be executed â any steps available, which could be anything. Think of the subpipeline
               as the working parts of the pipeline, while the rest is all about how it is set up.</p>
                  <p>The list of elements that come before the subpipeline is short, which helps: <code>p:import</code>,
                  <code>p:import-functions</code>, <code>p:input</code>, <code>p:output</code>, <code>p:option</code> or
                  <code>p:declare-step</code>. Everything coming after is a step.</p>
                  <p>Within this set of elements (all preceding, none following the subpipeline) XProc further distinguishes
               between the <b>imports</b> for steps and functions, appearing first (elements <code>p:import</code> and
                  <code>p:import-functions</code>), to be followed by elements configuring the step:
                  <code>p:input</code>, <code>p:output</code>, <code>p:option</code> â elements together called the <a href="https://spec.xproc.org/3.0/xproc/#declare-pipelines">prologue</a>.</p>
                  <p>The prologue is used to define ports and options for the pipeline - the points of control for its
               interfaces. (Technically: runtime bindings, and parameter or option settings.) If only a single input is
               needed, a single input port (named <code>source</code>) can be assumed, so prologues can be empty (and
               invisible, or not there).</p>
                  <p>Following the prologue, a step may also have local step definitions (<code>p:declare-step</code>).</p>
                  <p>After imports, prologue and (optional) step declarations, the step sequence that follows comprises the <a href="https://spec.xproc.org/3.0/xproc/#dt-subpipeline">subpipeline</a>.</p>
                  <p>One other complication: among the steps in the subpipeline, <code>p:variable</code> (a variable
               declaration) and <code>p:documentation</code> (for out-of-band documentation) are also permitted â these
               are not properly steps, but can be useful to have with them.</p>
                  <p>In summary: any XProc pipeline, viewed as a step declaration, can have the following --</p>
                  <ul>
                     <li>Pipeline name and type assignment (if needed), given as attributes at the top</li>
                     <li>
                        <b>Imports</b>: step declarations, step libraries and functions to make available</li>
                     <li>The pipeline <b>prologue</b>: any of the elements named <code>p:input</code>, <code>p:output</code>
                  and <code>p:option</code>, defining this pipeline's ports and options<ul>
                           <li>If no ports are named, assume a single <code>source</code> primary input port, permitting a
                        single document</li>
                        </ul>
                     </li>
                     <li>Optionally (and not common): step declarations for local steps, appearing at
                     <code>p:declare-step</code>. Each of these will have its own name, type, prologue and steps</li>
                     <li>For this pipeline, one or more steps, called the <a href="https://spec.xproc.org/3.0/xproc/#dt-subpipeline">subpipeline</a>
                        <ul>
                           <li>Standard atomic and compound steps in XProc namespace (probably prefixed <code>p:</code>)</li>
                           <li>Imported steps - in their own namespaces (in this repository, prefixed <code>ox:</code>)</li>
                           <li>Variable declarations - <code>p:variable</code>
                           </li>
                        </ul>
                     </li>
                     <li>Finally, as noted above, <code>p:documentation</code> can appear anywhere in a pipeline, but it will
                  be ignored except when appearing inside <code>p:inline</code>. What to do with these is a topic to be
                  covered later.</li>
                  </ul>
                  <p>NB: the pipelines run so far have XML comments demarcating the prologue from the steps</p>
               </section>
               <section>
                  <h3>Atomic and compound steps</h3>
                  <p>Given an understanding of the organization of an XProc pipeline, all that remains to understand of its
               syntax is the steps themselves, which follow a common pattern. Briefly put, atomic steps are any steps
               you use by simply invoking it with inputs and options: its logic is self-contained, and the operation it
               carries out is (at least conceptually) <q>single</q>. Compound steps, instead, are used to deploy more
               than one subpipeline, with settings determined dynamically for the step.</p>
                  <p>Fortunately XProc keeps things simple by providing only a few compound steps supporting the identified
               range of needs â and no way for users to define their own. This does not prove to be a practical
               limitation, since atomic steps can have multiple inputs and outputs, distinguished by type and role, and
               indeed since atomic steps used in a pipeline can be defined with compound steps in their own
               subpipelines, either externally or even within the same step declaration.</p>
                  <p>Here are all the compound steps. All others are atomic steps.</p>
                  <ul>
                     <li>
                        <a href="https://spec.xproc.org/3.0/xproc/#p.for-each"
                           style="color: rgb(3, 69, 117); text-decoration: none; border-bottom: 1px solid rgb(112, 112, 112); padding: 0px 1px; margin: 0px -1px; font-family: sans-serif; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: -120px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal;">
                           <code class="tag-element language-construct"
                                 style="font-family: Consolas, Monaco, &#34;Andale Mono&#34;, monospace; font-size: 16px; break-inside: avoid; hyphens: none; text-transform: none; text-align: left; white-space: pre; color: black; text-shadow: white 0px 1px; direction: ltr; word-spacing: normal; word-break: normal; tab-size: 4; padding: 0.1em; border-radius: 0.3em;">p:for-each</code>
                        </a>
                  - produce subpipeline results for each member of a sequence of inputs (documents or nodes)</li>
                     <li>
                        <a href="https://spec.xproc.org/3.0/xproc/#p.if"
                           style="color: rgb(3, 69, 117); text-decoration: none; border-bottom: 1px solid rgb(112, 112, 112); padding: 0px 1px; margin: 0px -1px; font-family: sans-serif; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: -120px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal;">
                           <code class="tag-element language-construct"
                                 style="font-family: Consolas, Monaco, &#34;Andale Mono&#34;, monospace; font-size: 16px; break-inside: avoid; hyphens: none; text-transform: none; text-align: left; white-space: pre; color: black; text-shadow: white 0px 1px; direction: ltr; word-spacing: normal; word-break: normal; tab-size: 4; padding: 0.1em; border-radius: 0.3em;">p:if</code>
                        </a>
                  - execute a subpipeline conditionally</li>
                     <li>
                        <a href="https://spec.xproc.org/3.0/xproc/#p.choose"
                           style="color: rgb(3, 69, 117); text-decoration: none; border-bottom: 1px solid rgb(112, 112, 112); padding: 0px 1px; margin: 0px -1px; font-family: sans-serif; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: -120px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal;">
                           <code class="tag-element language-construct"
                                 style="font-family: Consolas, Monaco, &#34;Andale Mono&#34;, monospace; font-size: 16px; break-inside: avoid; hyphens: none; text-transform: none; text-align: left; white-space: pre; color: black; text-shadow: white 0px 1px; direction: ltr; word-spacing: normal; word-break: normal; tab-size: 4; padding: 0.1em; border-radius: 0.3em;">p:choose</code>
                        </a>
                  - execute a subpipeline conditionally (<code>switch/case</code> operator)</li>
                     <li>
                        <a href="https://spec.xproc.org/3.0/xproc/#p.group"
                           style="color: rgb(3, 69, 117); text-decoration: none; border-bottom: 1px solid rgb(112, 112, 112); padding: 0px 1px; margin: 0px -1px; font-family: sans-serif; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: -120px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal;">
                           <code class="tag-element language-construct"
                                 style="font-family: Consolas, Monaco, &#34;Andale Mono&#34;, monospace; font-size: 16px; break-inside: avoid; hyphens: none; text-transform: none; text-align: left; white-space: pre; color: black; text-shadow: white 0px 1px; direction: ltr; word-spacing: normal; word-break: normal; tab-size: 4; padding: 0.1em; border-radius: 0.3em;">p:group</code>
                        </a>
                  - group a subpipeline (step sequence) into a single logical step</li>
                     <li>
                        <a href="https://spec.xproc.org/3.0/xproc/#p.viewport"
                           style="color: rgb(3, 69, 117); text-decoration: none; border-bottom: 1px solid rgb(112, 112, 112); padding: 0px 1px; margin: 0px -1px; font-family: sans-serif; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: -120px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal;">
                           <code class="tag-element language-construct"
                                 style="font-family: Consolas, Monaco, &#34;Andale Mono&#34;, monospace; font-size: 16px; break-inside: avoid; hyphens: none; text-transform: none; text-align: left; white-space: pre; color: black; text-shadow: white 0px 1px; direction: ltr; word-spacing: normal; word-break: normal; tab-size: 4; padding: 0.1em; border-radius: 0.3em;">p:viewport</code>
                        </a>
                  - reproduce outputs, except splicing subpipeline results in place of matched nodes (elements) in the
                  input</li>
                     <li>
                        <a href="https://spec.xproc.org/3.0/xproc/#p.try"
                           style="color: rgb(3, 69, 117); text-decoration: none; border-bottom: 1px solid rgb(112, 112, 112); padding: 0px 1px; margin: 0px -1px; font-family: sans-serif; font-size: medium; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: -120px; text-transform: none; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; white-space: normal;">
                           <code class="tag-element language-construct"
                                 style="font-family: Consolas, Monaco, &#34;Andale Mono&#34;, monospace; font-size: 16px; break-inside: avoid; hyphens: none; text-transform: none; text-align: left; white-space: pre; color: black; text-shadow: white 0px 1px; direction: ltr; word-spacing: normal; word-break: normal; tab-size: 4; padding: 0.1em; border-radius: 0.3em;">p:try</code>
                        </a>
                  - execute a subpipeline, and deliver its results, or if it fails, a fallback subpipeline given in a
                     <code>p:catch</code>
                     </li>
                  </ul>
                  <p>Additionally to these elements, XProc subpipelines may contain variable declarations and documentation,
               as noted below.</p>
               </section>
               <section>
                  <h3>Namespaces and extension steps</h3>
                  <p>We recognize steps because we either recognize them by name - for standard steps in the <code>p:</code>
               (XProc) namespace such as <code>p:filter</code> and <code>p:add-attribute</code> - or because we do not.
               Extension steps in XProc take the form of elements in an extension namespace. Generally speaking, that
               is, any element not prefixed with <code>p:</code> is treated as out of scope for XProc and to be ignored,
               while subject to evaluation as an extension.</p>
                  <p>In an XProc pipeline (library or step declaration) one may also see a namespace <code>c:</code>. TODO -
               come back to</p>
                  <details>
                     <summary>Question: Where are extension steps used in the XProcs run so far?</summary>
                     <p>Answer: The <a href="./../../../smoketest/TEST-XSPEC.xpl">XSpec smoke test</a> calls an extension
                  step named <code>ox:execute-xspec</code>, defined in an imported pipeline. In this document, the
                  prefix <code>ox</code> is bound to a utility namespace,
                     <code>http://csrc.nist.gov/ns/oscal-xproc3</code>.</p>
                  </details>
               </section>
               <section>
                  <h3>Schema for XProc 3.0</h3>
                  <p>See the <a href="unpack_599_src.html"
                        class="LessonUnit">599-level coverage in this lesson unit</a> for a discussion of the
               schema for XProc.</p>
               </section>
            </section>
            <section>
               <h2>Take note</h2>
               <section>
                  <h3>Where are these downloads coming from?</h3>
                  <p>Pipelines can use a few different strategies for resource acquisition, depending on the case, and on
               where and in what form the resource is available. (Sometimes a file on Github is easiest to download
               "raw", sometimes an archive is downloaded and opened, and so on.) For now, it is not necessary to
               understand details in every case, only to observe the variation and range. (With more ideas welcome.
               Could XProc be used to build a <q>secure downloader</q> that knows how, for example, to compare
               hashes?)</p>
                  <p>Wherever you see <code>href</code> attributes, take note.</p>
                  <p>Since <code>href</code> is how XProc <q>sees</q> the world, either to read data in or to write data out,
               this attribute is a reliable indicator of an assumed feature, often a dependency of some kind. For
               example, a download will not succeed if the resource indicated by the <code>href</code> for the download
               returns an error, or nothing. In XProc, <code>href</code> attribute settings are the <i>points of
                  control</i> for interaction between an XProc pipeline, and its runtime environment.</p>
                  <p>Useful detail: where XProc has <code>p:store href="some-uri.file"</code>, the <code>href</code> is read
               by the processor as the intended location for storage of pipeline data, that is, for a <i>write</i>
               operation. In other cases <code>href</code> is always an argument for a <i>read</i> operation.</p>
               </section>
               <section>
                  <h3>Syntax tips</h3>
                  <p>In XPath syntax, <code>$foo</code> (a name with a <code>$</code> prefixed) indicates a <b>variable
                  reference</b> named (in this case) <q>foo</q>. XProc also uses a <i>value expansion syntax</i>
                  (either<i>text value syntax</i> or <i>attribute value syntax</i>) using curly braces - so syntax such
               as <code>href="{$some-xml-uri}"</code> is not uncommon. Depending on use, this would mean <q>read [or
                  write] to the URI given by <code>$some-xml-uri</code>
                     </q>.</p>
                  <p>An XProc developer always knows where <code>href</code> is used in a pipeline, and how to test for and
               update its use. As always with syntax, the easiest way to learn it is to try making changes and observing
               outcomes.</p>
               </section>
            </section>
            <section>
               <h2>Exercise: Knowing what to look for</h2>
               <p>The last lesson unit already provided an opportunity to alter pipelines and see how they fail when not
            encoded correctly â when <q>broken</q>, any way we can think of breaking them. (Then we had an opportunity
            to put them back.)</p>
               <p>After reading this page, do this again, except focusing on a few key areas:</p>
               <ul>
                  <li>Keep in mind how inputs (source data) for your pipeline can be provided either with <code>p:input</code>
               (part of the pipeline prologue) or directly by <code>p:load</code> (a step). When you see a document
               referenced on either <code>p:document</code> or <code>p:load</code>, by means of <code>href</code>, you
               are looking at a call either to an XML document, or some other data instance (e.g.: text file; JSON data
               instance). (One difference is that <code>p:document</code> bindings provided to <code>p:input</code> can
               be overridden but <code>p:load</code> says what it says.) These inputs are ordinarily dependencies for
               the pipeline. Change or override them, and you change the inputs provided.</li>
                  <li>
                     <code>@href</code> attributes on <code>p:store</code> (in contrast to <code>p:load</code>) are equally
               important, but for the opposite reason: <code>p:store</code> is one of the ways XProc offers to
                  <q>write</q> or save out its processing results, to the location given (in a writeable file system).
               The other way is binding to a <code>p:output</code> port and doing something with that.</li>
                  <li>Detail: while <code>p:input/p:document</code> is a way of providing inputs, you aren't likely to see
                  <code>p:output/p:document</code>. The XProc specication clarifies the reason why (at the end of the
               section <a href="https://spec.xproc.org/3.0/xproc/#p.output">p:output</a>) this pattern would be used
               only for very special purposes, if at all. Defining an output <b>port</b>, <code>p:output</code>, when
               given, shows not what will or should happen with process results (outputs), but rather exactly what kinds
               of outputs are available, with the names and configurations including which results they capture. If such
               a port is not connected to a processing result but to a static resource, the static resource is what will
               appear there.</li>
                  <li>Reverse this logic and you can see that a pipeline with no <code>p:output</code> must somewhere among
               its steps have one or more <code>p:store</code> steps, since these are the only ways results are made
               available externally to the pipeline runtime.</li>
                  <li>For security analysts: yes, this last point is consequential for purposes of auditing and assessing
               vulnerabilities in and with XProc. A pipeline with no <code>p:store</code> has no effects on a file
               system where it runs; a pipeline with no output ports exposes no results (for a calling process to
               receive) - so to have neither is effectively to have no effects anywhere.</li>
                  <li>Both <code>p:load</code> and <code>p:store</code> are commonly provided with <code>@message</code>
               attributes, which are used to produce console messages (in a tool like Morgana) when steps in a
               subpipeline are executed.</li>
               </ul>
               <p>After breaking anything, restore it to working order. Create modified copies of any pipelines for further
            analysis and discussion.</p>
               <p>Concept: copy and change one of the pipelines provided to acquire a software library or resource of your
            choice.</p>
            </section>
         </section>
         <section class="unit"
                  id="unpack_599">
            <h1>599: More context</h1>
            <p>More in depth.</p>
            <section>
               <h2>Goals</h2>
               <ul>
                  <li>Consider XProc in its operational context including available <b>standards</b> and applicable
                  <b>requirements</b>, both generalized and local</li>
                  <li>Learn or relearn some deep XML history including alternative approaches</li>
                  <li>Inform your capability to assess the utility and appropriateness of XProc in particular and XML in
               general, for a given problem or domain</li>
               </ul>
            </section>
            <section>
               <h2>Resources</h2>
               <p>The same pipelines you ran in setup: <a href="../setup/setup_101_src.html"
                     class="LessonUnit">Setup 101</a>.</p>
               <p>Also, <a href="https://xproc.org">XProc.org dashboard page</a>
               </p>
               <p>Also, XProc index materials produced in this repository: <a href="../../../projects/xproc-doc/readme.md">XProc
               docs</a>
               </p>
            </section>
            <section>
               <h2>Prerequisites</h2>
               <p>Same as <a href="../setup/setup_101_src.html"
                     class="LessonUnit">Setup 101</a>. Prior exercises, or the practical equivalent, are
            assumed.</p>
            </section>
            <section>
               <h2>XProc schema</h2>
               <p>A schema for the XProc language, considered as core steps (compound and atomic) plus optional
            community-defined steps, is referenced from the <a href="https://spec.xproc.org/3.0/xproc/#ancillary-files">XProc Specification</a>. <a href="https://spec.xproc.org/3.0/xproc/xproc30.rng">This RNG schema</a> is
            very useful.</p>
               <p>It may often be considered gratuitous to validate XProc files against a schema, when the application (for
            us, Morgana) must in any case take responsibility for conformance issues, as it sees fit. The reference
            schema becomes useful if we find or suspect bugs in Morgana, but until then it need not have any direct role
            in any runtime operation.</p>
               <p>Nevertheless the RNG schema still serves as a reference and an object for querying -- queries whose results
            tell us about XProc. <a href="../../GRAB-XPROC-RESOURCES.xpl">A pipeline</a> for acquiring both RNG schema
            and its RNC (compact syntax) variant is provided for interest and possible later use.</p>
            </section>
            <section>
               <h2>XPath</h2>
               <p>Like other XDM-based technologies, XProc embeds and incorporates XPath, an expression language for XML.
            XPath 3.0 is a functional language in its own right, although not designed for end-to-end processing of
            encoded source data into encoded results, but only for certain critical operations that ordinarily need to
            be performed within such end-to-end processing. Importantly, XPath is defined not in terms of any data
            notation (such as XML syntax or any other) but rather against an <i>abstract data object</i>, namely an <a href="https://www.w3.org/TR/xpath-datamodel/">XDM</a> instance (XML data model), a putative information
            object that may be provided to the system by parsing an XML (syntax) instance, or by other means. As the
            query language for <a href="https://www.w3.org/TR/xpath-datamodel/">XDM</a> and the basis for XQuery, <a href="https://www.w3.org/TR/xpath-31/">XPath</a> is the <q>other half</q> of the data model, which any
            architect of a system using this technology must know. Learning XPath equips you mentally for dealing with
            the XDM in XQuery, XSLT, XProc or anywhere you find it.</p>
               <p>For those not already familiar with XPath, on line resources can be helpful. Keep in mind that <a href="https://www.w3.org/TR/xpath-31/">XPath 3.1</a> outstrips XPath 1.0 in many important respects.</p>
               <section>
                  <h3>Documents and data</h3>
                  <p>One of the more important features of XPath and the XDM is that they are designed not only to meet needs
               for the representation and transmission of structured data. A specialized class of data formats has
               evolved that represent information in ways that are not <q>unstructured</q>, but that contrast with more
               common or usual structures of data formats, whether they be tabular data, serialization formats for
               object models, or some other regular (formalized and codified) arrangement. One might say <q>common</q>
               or <q>usual</q> with reservation, since of course documents are not uncommon where they are common.</p>
                  <p>We see a great deal of structured data these days if only because it is so easy to make structured data
               with machines, and we now have the machines. What remains difficult is to translate what has not been
               created by a machine, into a form that a machine can <q>recognize</q>, or rather into a form we can
               recognize in and with the machine, without destroying it.</p>
                  <p>So documents are called <q>unstructured</q> but they might better be called <q>relatively irregular</q>,
               meaning not that they have no structure, but that each one is structured in itself, and moreover, likely
               to be incompatible or not fully compatible with encodings designed to capture other structures.</p>
                  <p>And to the extent this is the case, any encoding capable of describing documents must have the capability
               of supporting each document's own distinctive structure and organization, whether that be due to its
               family (what is called a <b>document type</b>) or an expression of its own intrinsic logic. The format
               must be not only structured, but <i>structurable</i>, and its structures must to some extent be capable
               of self-description â combining data with metadata.</p>
                  <p>And this is to give no consideration to the fact that these structures can be described at <i>multiple
                  levels</i> of generality or specificity with regard to either their supposed semantics, or their
               configuration in operation.</p>
                  <p>Documentary data formats especially markup formats are designed to work in this in-between space.</p>
                  <p>And so we get XPath - a query syntax which permits working with an organized structure of a particular kind
               (an <i>XDM document tree</i>), which in turn is designed for handling the combination of <i>highly
                  regular</i> and <i>quite irregular</i> data structures that characterize information sets we (loosely)
               call <b>documentary</b>.</p>
                  <p>A definition for what is a document is out of scope for this tutorial â an interesting topic but not only a
               technical one.</p>
               </section>
            </section>
            <section>
               <h2>Survey of XProc elements</h2>
               <p>All elements defined by XProc are listed in this analytical breakout.</p>
               <p>TODO - tbd - reformat this table for legibility (CSS grids); validate its completeness against XProc
            RNG?</p>
               <p>NB - XProc 3.1 makes this a moving target.</p>
               <p>NB also â see Erik Siegel's <a href="https://xprocref.org/index.html">XProcRef</a> indexing project for more
            detailed summaries.</p>
               <table>
                  <tbody>
                     <tr>
                        <th>Function</th>
                        <th>XProc elements / p: namespace</th>
                     </tr>
                     <tr>
                        <td>Documentation</td>
                        <td>
                           <code>p:documentation</code>, , </td>
                     </tr>
                     <tr>
                        <td>Top-level</td>
                        <td>
                           <code>p:declare-step</code>, <code>p:library</code>
                        </td>
                     </tr>
                     <tr>
                        <td>Imports</td>
                        <td>
                           <code>p:import</code>, <code>p:import-functions</code>
                        </td>
                     </tr>
                     <tr>
                        <td>Prologue</td>
                        <td>
                           <code>p:input</code>, <code>p:output</code>, <code>p:option</code>
                        </td>
                     </tr>
                     <tr>
                        <td>Compound steps</td>
                        <td>
                           <code>p:for-each</code>, <code>p:viewport</code>, <code>p:choose</code>, <code>p:when</code>,
                     <code>p:otherwise</code>, <code>p:if</code>, <code>p:group</code>, <code>p:try</code>,
                     <code>p:catch</code>, <code>p:finally</code>, <code>p:run</code>, <code>p:run-input</code>,
                     <code>p:run-option</code>
                        </td>
                     </tr>
                     <tr>
                        <td>Atomic steps - core - XML</td>
                        <td>
                           <code>p:add-attribute</code>, <code>p:add-xml-base</code>, <code>p:delete</code>,
                     <code>p:filter</code>, <code>p:identity</code>, <code>p:insert</code>,
                     <code>p:label-elements</code>, <code>p:make-absolute-uris</code>, <code>p:namespace-delete</code>,
                     <code>p:namespace-rename</code>, <code>p:pack</code>, <code>p:rename</code>,
                  <code>p:replace</code>, <code>p:set-attributes</code>, <code>p:uuid</code>, <code>p:unwrap</code>,
                     <code>p:wrap-sequence</code>, <code>p:wrap</code>, <code>p:xinclude</code>, <code>p:xquery</code>,
                     <code>p:xslt</code>
                        </td>
                     </tr>
                     <tr>
                        <td>Atomic steps - core - zipping</td>
                        <td>
                           <code>p:archive</code>, <code>p:archive-manifest</code>, <code>p:unarchive</code>,
                     <code>p:uncompress</code>
                        </td>
                     </tr>
                     <tr>
                        <td>Atomic steps - core - JSON</td>
                        <td>
                           <code>p:json-join</code>, <code>p:json-merge</code>, <code>p:set-properties</code>
                        </td>
                     </tr>
                     <tr>
                        <td>Atomic steps - core - plain text</td>
                        <td>
                           <code>p:string-replace</code>, <code>p:text-count</code>, <code>p:text-head</code>,
                     <code>p:text-join</code>, <code>p:text-replace</code>, <code>p:text-sort</code>,
                     <code>p:text-tail</code>
                        </td>
                     </tr>
                     <tr>
                        <td>Atomic steps - core - utility</td>
                        <td>
                           <code>p:cast-content-type</code>, <code>p:compare</code>, <code>p:compress</code>,
                     <code>p:count</code>, <code>p:error</code>, <code>p:hash</code>, <code>p:http-request</code>,
                     <code>p:load</code>, <code>p:sink</code>, <code>p:split-sequence</code>, <code>p:store</code>,
                     <code>p:www-form-urldecode</code>, <code>p:www-form-urlencode</code>
                        </td>
                     </tr>
                     <tr>
                        <td>Atomic steps - optional - file system</td>
                        <td>
                           <code>p:directory-list</code>, <code>p:file-copy</code>, <code>p:file-delete</code>,
                     <code>p:file-info</code>, <code>p:file-mkdir</code>, <code>p:file-move</code>,
                     <code>p:file-create-tempfile</code>, <code>p:file-touch</code>
                        </td>
                     </tr>
                     <tr>
                        <td>Atomic steps - optional - validation</td>
                        <td>
                           <code>p:validate-with-nvdl</code>, <code>p:validate-with-relax-ng</code>,
                     <code>p:validate-with-schematron</code>, <code>p:validate-with-xml-schema</code>,
                     <code>p:validate-with-json-schema</code>
                        </td>
                     </tr>
                     <tr>
                        <td>Other optional steps</td>
                        <td>
                           <code>p:os-info</code>, <code>p:os-exec</code>, <code>p:css-formatter</code>,
                     <code>p:xsl-formatter</code>, <code>p:markdown-to-html</code>
                        </td>
                     </tr>
                     <tr>
                        <td>Variable declaration</td>
                        <td>
                           <code>p:variable</code>
                        </td>
                     </tr>
                     <tr>
                        <td>Connectors</td>
                        <td>
                           <code>p:with-input</code>, <code>p:with-option</code>, <code>p:pipe</code>, <code>p:pipeinfo</code>,
                     <code>p:document</code>, <code>p:inline</code>, <code>p:empty</code>
                        </td>
                     </tr>
                  </tbody>
               </table>
            </section>
            <section>
               <h2>XML time line</h2>
               <p>[TODO: complete this, or move it, or both]</p>
               <table>
                  <thead>
                     <tr>
                        <th>Year</th>
                        <th>Publication</th>
                        <th>Capabilities</th>
                        <th>Processing frameworks</th>
                        <th>Platforms</th>
                     </tr>
                  </thead>
                  <tbody>
                     <tr>
                        <td>1987</td>
                        <td>SGML (ISO-IEC 8879-1)</td>
                        <td>parsing logic; schema validation; configurable syntax; (implicit) tree of elements and
                     attributes</td>
                        <td>Proprietary stacks</td>
                        <td>Mainframes, workstations</td>
                     </tr>
                     <tr>
                        <td>1998</td>
                        <td>XML 1.0</td>
                        <td>standard syntax</td>
                        <td>Batch processing, shell scripts, <code>make</code>
                        </td>
                        <td>Mainframes, workstations, PCs (x86 generation)</td>
                     </tr>
                     <tr>
                        <td>1996</td>
                        <td>Unicode 2.0</td>
                        <td>standard character sets</td>
                        <td>Support for Unicode is slow to come</td>
                        <td>
                        </td>
                     </tr>
                     <tr>
                        <td>1999</td>
                        <td>XPath 1.0, XSLT 1.0</td>
                        <td>basic tree querying and transformations (down hill); functional support for namespaces</td>
                        <td>Web browsers? (some, sort of)</td>
                        <td>
                        </td>
                     </tr>
                     <tr>
                        <td>2000</td>
                        <td>
                        </td>
                        <td>XML-configured software builds</td>
                        <td>Apache Ant</td>
                        <td>Java</td>
                     </tr>
                     <tr>
                        <td>
                        </td>
                        <td>XQuery 1.0</td>
                        <td>
                        </td>
                        <td>Perl, Python, Java APIs / integration</td>
                        <td>
                        </td>
                     </tr>
                     <tr>
                        <td>
                        </td>
                        <td>XPath 2.0</td>
                        <td>
                        </td>
                        <td>Server frameworks (Apache Cocoon)</td>
                        <td>
                        </td>
                     </tr>
                     <tr>
                        <td>2001</td>
                        <td>XML Schema Definition language (XDM)</td>
                        <td>Standardizes atomic data types (foundations of XSD); namespace-based validation (RNG also offers
                     this, 2001-2002)</td>
                        <td>
                        </td>
                        <td>
                        </td>
                     </tr>
                     <tr>
                        <td>2003-2004</td>
                        <td>W3C Document Object Model (DOM)</td>
                        <td>API for HTML and XML documents</td>
                        <td>
                        </td>
                        <td>
                        </td>
                     </tr>
                     <tr>
                        <td>2005</td>
                        <td>
                           <q>The XML data model</q> (W3C)</td>
                        <td>An essay</td>
                        <td>
                        </td>
                        <td>
                        </td>
                     </tr>
                     <tr>
                        <td>
                        </td>
                        <td>
                        </td>
                        <td>
                        </td>
                        <td>
                        </td>
                        <td>
                        </td>
                     </tr>
                     <tr>
                        <td>2007</td>
                        <td>XSLT 2.0</td>
                        <td>transformations (up hill)</td>
                        <td>XProc 1.0</td>
                        <td>
                        </td>
                     </tr>
                     <tr>
                        <td>
                        </td>
                        <td>XDM (XPath/XQuery data model)</td>
                        <td>unification</td>
                        <td>
                        </td>
                        <td>Client- and server-side XML processing stacks</td>
                     </tr>
                     <tr>
                        <td>
                        </td>
                        <td>
                        </td>
                        <td>
                        </td>
                        <td>XQuery+XSLT in eXist-db or BaseX (XQuery engines)</td>
                        <td>
                        </td>
                     </tr>
                     <tr>
                        <td>
                        </td>
                        <td>XPath 3.0</td>
                        <td>
                        </td>
                        <td>
                        </td>
                        <td>
                        </td>
                     </tr>
                     <tr>
                        <td>
                        </td>
                        <td>XPath 3.1</td>
                        <td>higher-order functions, map and array objects</td>
                        <td>
                        </td>
                        <td>
                        </td>
                     </tr>
                     <tr>
                        <td>
                        </td>
                        <td>XProc 1.0</td>
                        <td>
                        </td>
                        <td>
                        </td>
                        <td>
                        </td>
                     </tr>
                     <tr>
                        <td>2017</td>
                        <td>XSLT 3.0/3.1</td>
                        <td>JSON harmonization, functions as arguments</td>
                        <td>
                        </td>
                        <td>
                        </td>
                     </tr>
                     <tr>
                        <td>
                        </td>
                        <td>XProc 3.0</td>
                        <td>
                        </td>
                        <td>
                        </td>
                        <td>
                        </td>
                     </tr>
                     <tr>
                        <td>2022</td>
                        <td>Unicode 15.0</td>
                        <td>
                        </td>
                        <td>
                        </td>
                        <td>
                        </td>
                     </tr>
                  </tbody>
               </table>
               <p>The technologies have been in constant use over this period.</p>
               <p>Historically, the requirements of processing frameworks have often been met by software developers' build
            utilities (for example, GNU <code>make</code> or Apache Ant). This is not an accident: in certain respects,
            a publishing framework can be considered as a <q>documentary build</q> at a higher level.</p>
            </section>
            <section>
               <h2>XPath illustrative examples</h2>
               <p>This is not the place to learn XPath, but a selection of XPath expressions can offer a hint of its
            capabilities.</p>
               <table>
                  <thead>
                     <tr>
                        <th>XPath</th>
                        <th>Returns</th>
                        <th>XPath long (explicit) notation</th>
                     </tr>
                  </thead>
                  <tbody>
                     <tr>
                        <td>
                           <code>/html</code>
                        </td>
                        <td>An XML document root (top-level) element named <code>html</code> (subject to namespace
                     resolution)</td>
                        <td>
                           <code>/child::html</code>
                        </td>
                     </tr>
                     <tr>
                        <td>
                           <code>//p</code>
                        </td>
                        <td>All the elements named <code>p</code> in the document</td>
                        <td>
                           <code>/descendant-or-self::element()/child::p</code>
                        </td>
                     </tr>
                     <tr>
                        <td>
                           <code>//seg[@type='null']</code>
                        </td>
                        <td>All the elements named <code>seg</code> with an attribute <code>type</code> with value
                        <code>null</code>
                        </td>
                        <td>
                           <code>/descendant-or-self::element()/child::seg[attribute::type='null']</code>
                        </td>
                     </tr>
                     <tr>
                        <td>
                           <code>/*</code>
                        </td>
                        <td>Any document (rather, any element at the top of a document) - <code>*</code> is a wildcard
                     character</td>
                        <td>
                           <code>/child::element()</code>
                        </td>
                     </tr>
                     <tr>
                        <td>
                           <code>/section[exists(.//table)]</code>
                        </td>
                        <td>An element inside the top-level element, named <code>section</code>, that contains a
                        <code>table</code> element anywhere inside it</td>
                        <td>
                           <code>/child::section[exists(self::node()/descendant-or-self::element()/child::table)]</code>
                        </td>
                     </tr>
                     <tr>
                        <td>
                           <code>/descendant::p[10]</code>
                        </td>
                        <td>The tenth <code>p</code> element in the document</td>
                        <td>
                           <code>/descendant::p[position() eq 10]</code>
                        </td>
                     </tr>
                     <tr>
                        <td>
                           <code>//p[10]</code>
                        </td>
                        <td>All <code>p</code> elements, that are the tenth <code>p</code> inside their respective
                     parents</td>
                        <td>
                           <code>/descendant-or-self::element()/child::p[position() eq 10]</code>
                        </td>
                     </tr>
                     <tr>
                        <td>
                           <code>//section[count(.//p) gt 10]</code>
                        </td>
                        <td>All <code>section</code> elements that contain more than 10 <code>p</code> elements, at any
                     depth</td>
                        <td>
                           <code>/child::section[count(self::node()/descendant-or-self::element()/child::p) gt
                     10]</code>
                        </td>
                     </tr>
                  </tbody>
               </table>
            </section>
            <section>
               <h2>XML and the XDM: context and rationale</h2>
               <ul>
                  <li>Standard, non-proprietary and freely available without restriction</li>
                  <li>Consistently and repeatedly shown to be capable at scale (size/complexity)</li>
                  <li>Supported by commodity tools, easing problem of proprietary product dependencies</li>
               </ul>
               <p>The technologies we rely on share a common foundation in XML and XDM  (the XML data model), technologies
            developed under the auspices of the World Wide Web Consortium. </p>
               <p>These are supported by commodity tools that are freely available to use without restriction, an important
            qualification for this distribution, which has a prior commitment <i>not to endorse particular technological
               solutions to any problem</i>, however posed or circumscribed. Accordingly, solutions here are not offered
            as recommendations, but rather as stipulations of (minimum) viable functionality in tools or capabilities,
            and not only using tools as <q>black boxes</q>, but under control and conformant to external specifications
            â i.e., standards.</p>
               <p>Users of these tools should keep in mind the model whereby we imagine the viability of a tools market and
            ecosystem that enables both large and small software developers â including independent developers, academic
            researchers, and students â to participate meaningfully, finding an appropriate value or service proposition
            to support immediate and long-term goals. Translated, this means the tools must be capable enough for
            industrial use at scale, while they must also <q>scale down</q> to demonstration or classroom use.</p>
               <p>In web standards including HTML and Javascript (ECMAScript) we arguably have the beginnings of such an
            ecosystem, while it is also contested and turbulent. Within the publishing sector more broadly and
            intersecting with the web, the XML family of standards arguably provides the best demonstration of complete
            or near-complete capabilities at least with respect to the harder problems of document processing.</p>
               <ul>
                  <li>XSLT up to <a href="https://www.w3.org/TR/xslt-30/">XSLT 3.0</a> (in <a href="https://www.saxonica.com/welcome/welcome.xml">Saxon</a>)</li>
                  <li>
                     <a href="https://www.w3.org/TR/xquery-31/">XQuery</a> (in Saxon)</li>
                  <li>
                     <a href="https://github.com/Schematron">Schematron</a> (in <a href="https://github.com/schxslt/schxslt">SchXSLT</a>, an open-source implementation
               in XSLT of <a href="https://schematron.com/">Schematron</a> including the <a href="https://www.iso.org/obp/ui/#iso:std:iso-iec:19757:-3:ed-3:v1:en">ISO/IEC 19757-3</a>
               specification</li>
                  <li>
                     <a href="https://github.com/xspec/xspec">XSpec</a>, a community-maintained XSLT-based framework for
               test-driven development, supporting testing XSLT, XQuery and Schematron</li>
               </ul>
               <p>Since they are known to be highly conformant to their respective specifications as well as well tested,
            these tools provide a useful functional baseline for evaluating other tooling that addresses the same
            functional requirements.</p>
               <p>They are also, relatively speaking, <i>mature</i> technologies, at least in comparison to similar
            offerings.</p>
               <p>And when XProc works, we also have the functional underpinnings we need for comparing - for example -
            different XSLT implementations.</p>
               <p>Initiated in 1996, XML continues to be generative in 2024.</p>
            </section>
            <section>
               <h2>Exercise: Discussion board</h2>
               <p>Create or contribute to a Github discussion board offering perspective or (especially!) relevant information
            or experience on any of the larger questions.</p>
            </section>
         </section>
      </section>
      <section class="lesson"
               id="oscal-convert">
         <section class="unit"
                  id="oscal-convert_101">
            <h1>101: OSCAL from XML to JSON and back</h1>
            <section>
               <h2>Goals</h2>
               <p>Learn how OSCAL data can be converted between JSON and XML formats, using XProc.</p>
               <p>Learn something about potential problems and limitations when doing this, and about how to detect, avoid,
            prevent or mitigate them.</p>
            </section>
            <section>
               <h2>Prerequisites</h2>
               <p>You have succeeded in prior exercises, including tools installation and setup.</p>
            </section>
            <section>
               <h2>Resources</h2>
               <p>This unit relies on the <a href="../../../projects/oscal-convert/readme.md">oscal-convert project</a> in
            this repository, with its files. Like all projects in the repo, it aims to be reasonably self-contained and
            self-explanatory. Use your search engine and XProc resources to learn background and terminology.</p>
               <p>Also like other projects, there are preliminaries for acquiring resources, along with pipelines to run.</p>
            </section>
            <section>
               <h2>Step zero: an identity pipeline</h2>
            </section>
            <section>
               <h2>Step one: convert some OSCAL XML into OSCAL JSON</h2>
               <p>
                  <a href="../../../projects/oscal-convert/GRAB-RESOURCES.xpl">An acquisition pipeline</a> in the project folder
            collects some OSCAL onto the local system, where it can be managed, easily inspected, controlled, and edited
            if necessary.</p>
               <p>TBD / this all incoherent so far</p>
               <section>
                  <h3>The playing field is the internet</h3>
                  <p>Keep in mind that XProc in theory, and your XProc engine in practice, may read its inputs using whatever
               protocols it supports, while the <code>file</code> and <code>http</code> protocols are required for
               conformance, and work as they do on the Worldwide Web. Of course, permissions must be in place to read
               and write files from and into file system locations. But when authentication is configured or resources
               are openly available, using <code>http</code> to reach resources or sources can be a very convenient
               option.</p>
                  <p>Keep in mind that where resources take the form of serializations of structured data such as XML or JSON,
               what is <q>read</q> (in the sense of presented to a consuming application) may not be quite exactly the
               same data serialization, since as a serialization, it is parsed into an object or representation of some
               kind, </p>
               </section>
               <p>TBD - TODO - question - how many and of what sort of source data files - so far there is only the cat
            catalog</p>
               <ul>
                  <li>Converting local XML to JSON with a local XSLT</li>
                  <li>Converting local data using a remote XSLT</li>
                  <li>Remote data with a local XSLT, writing locally - <a href="https://github.com/GSA/fedramp-automation/blob/master/dist/content/rev5/baselines/xml/FedRAMP_rev5_LOW-baseline-resolved-profile_catalog.xml">
                     </a>
                  </li>
               </ul>
            </section>
            <section>
               <h2>Step two: return trip</h2>
               <p>Two ways: separate pipeline; and single pipeline; also a 'switcher' pipeline?</p>
            </section>
            <section>
               <h2>What is this XSLT?</h2>
               <p>If your criticism of XProc so far is that it makes it look easy when it isn't, you have a point. Conversion
            from XML to JSON isn't free, assuming it works at all. In this case, the heavy lifting is done by the XSLT
            component - the Saxon engine invoked by the <code>p:xslt</code> step, applying logic defined in an XSLT
            stylesheet (aka transformation) stored elsewhere. It happens that a converter for OSCAL data is available in
            XSLT, so rather than having to confront this considerable problem ourselves, we drop that in.</p>
               <p>In later units we will see how using the XProc steps described, rudimentary data manipulations can be done
            using XProc by itself, without entailing the use of either XSLT or XQuery (another capability invoked with a
            different step). At the same time, while pipelines are based on the idea of passing data through a series of
            processes, there are many cases where logic is sufficiently complex that it becomes essential to maintain â
            and test â that logic externally from the XProc. At what point it becomes more efficient to encapsulate
            logic separately (whether by XSLT, XQuery or other means), depends very much on the case.</p>
               <p>The <code>p:xslt</code> pipeline step in particular is so important for real-world uses of XProc that it is
            introduced early, to show such black-box application. XProc also makes a fine environment for testing XSLT
            developed or acquired to handle specific tasks, a topic covered in more depth later. Indeed XSLT and XQuery
            being, like XProc itself, declarative languages, it makes sense to factor them out while maintaining easy
            access and transparency for analysis and auditing purposes.</p>
            </section>
            <section>
               <h2>What could possibly go wrong?</h2>
               <p>When coping with errors, syntax errors are relatively easy. But anomalous inputs, especially invalid inputs,
            can result in lost data. (A common reason data is not valid even when it appears to be is that it has
            foreign unknown contents, or contents out of place - the kinds of things that might fail to be converted.)
            The most important concern when engineering a pipeline is to see to it that no data quality problems are
            introduced inadvertantly. While in comparison to syntax or configuration problems, data quality issues can
            be subtle, there is also good news: the very same tools we use to process inputs into outputs, can also be
            used to test and validate data to both applicable standards and local rules.</p>
               <p>Generally speaking, OSCAL maintains <q>validation parity</q> between its XML and JSON formats with respect
            to their schemas. That is to say, the XSD (XML schema) covers essentially the same set of rules for OSCAL
            XML data as the JSON Schema does for OSCAL JSON data, accounting for differences between the two notations,
            the data models and how information is mapped into them. A consequence of this is that valid OSCAL data,
            either XML or JSON, can reliably be converted to valid data in the other notation, while invalid data may
            not be converted at all, resulting in gaps or empty results.</p>
               <p>For this and related reasons on open systems, the working principle in XML is often to formalize a model (to
            write and deploy a schema) as early as possible - or adopt a model already built - as a way to institute and
            enforce schema validation as a <b>prerequisite</b> and <b>primary requirement</b> for working with any data
            set. Validation against schemas is covered in a subsequent lesson unit (coming soon near you).</p>
               <section>
                  <h3>Intercepting errors</h3>
                  <p>One way to manage the problem of ensuring input quality is to validate on the way in, either as a
               dependent (prerequisite) process, or built into a pipeline. Whatever you want to do with invalid inputs,
               including ignoring them and producing warnings or runtime exceptions, can be defined in a pipeline much
               like anything else.</p>
                  <p>In the <a href="../../../projects/oscal-publish/publish-oscal-catalog.xpl">publishing demonstration project
                  folder</a> is an XProc that valides XML against an OSCAL schema, before formatting it. The same could
               be done for an XProc that converts the data into JSON - either or both before or after conversion.</p>
                  <p>Learn more about recognizing and dealing with errors in <a href="oscal-convert_102_src.html"
                        class="LessonUnit">Lesson
                  102</a>, or continue on to the next project, oscal-validate, for more on validation of documents and
               sets of documents.</p>
               </section>
            </section>
         </section>
         <section class="unit"
                  id="oscal-convert_102">
            <h1>102: Hands on data conversions</h1>
            <section>
               <h2>Goals</h2>
               <p>Learn how OSCAL data can be converted between JSON and XML formats, using XProc.</p>
               <p>Learn something about potential problems and limitations when doing this, and about how to detect, avoid,
            prevent or mitigate them.</p>
               <p>Work with XProc features designed for handling JSON data (XDM <b>map</b> objects that can be cast to
            XML).</p>
            </section>
            <section>
               <h2>Prerequisites</h2>
               <p>Run the pipelines described in <a href="https://github.com/usnistgov/oscal-xproc3/discussions/18">the 101
               Lesson</a>
               </p>
            </section>
            <section>
               <h2>Resources</h2>
               <p>Same as the <a href="oscal-convert_101_src.html"
                     class="LessonUnit">101 lesson</a>.</p>
            </section>
            <section>
               <h2>Probing error space - data conversions</h2>
               <p>Broadly speaking, problems encountered running these conversions fall into two categories, the distinction
            being simple, namely whether a bad outcome is due to an error in the processor and its logic, or in the data
            inputs provided. The term <q>error</q> here hides a great deal. So does <q>bad outcome</q>. One type of bad
            outcome takes the form of failures at runtime - the term <q>failure</q> again leaving questions open. Other
            bad outcomes are not detectable at runtime. If inputs are bad (inconsistent with stated contracts such as
            data validation), processes can run <i>correctly</i> and deliver incorrect results: correctly representing
            inputs, in their incorrectness. Again, the term <i>correct</i> here is underspecified and underdefined,
            except in the case.</p>
               <section>
                  <h3>Converting broken XML or JSON</h3>
               </section>
               <section>
                  <h3>Converting broken OSCAL</h3>
               </section>
               <section>
                  <h3>Converting not-OSCAL</h3>
               </section>
            </section>
            <section>
               <h2>XProc diagnostic how-to</h2>
               <section>
                  <h3>Emitting runtime messages</h3>
               </section>
               <section>
                  <h3>Saving out interim results</h3>
                  <p>
                     <code>p:store</code>
                  </p>
               </section>
            </section>
            <section>
               <h2>Validate early and often</h2>
            </section>
            <section>
               <h2>for 599: XProc for JSON</h2>
               <p>map objects; steps for working with them; interim p:store as debug method</p>
            </section>
            <section>
               <h2>for 599: YAML TODO</h2>
               <p>map objects; steps for working with them</p>
            </section>
            <section>
               <h2>for 599: XProc port bindings</h2>
               <p>This is actually a .bat or .sh exercise - write a script that invokes XProc with a binding to a runtime
            argument</p>
               <p>Thus, a script <code>convert-oscal-catalog-xml.sh mycatalog.xml</code> could produce
               <code>mycatalog.json</code> from <code>mycatalog.xml</code> etc.</p>
               <p>Such a script could live in the project directory - do we want an Issue for this work item? </p>
            </section>
            <section>
               <h2>for 599: URIs and URI schemes</h2>
               <p>see <a href="https://spec.xproc.org/master/head/xproc/#err.inline.D0012">
                  </a> it is up to implementations to
            define supported URI schemes  - also XML catalogs</p>
            </section>
            <section>
               <h2>for 599: round tripping as process test</h2>
            </section>
         </section>
      </section>
      <section class="lesson"
               id="oscal-validate">
         <section class="unit"
                  id="oscal-validate_101">
            <h1>101: Seeing valid OSCAL</h1>
            <section>
               <h2>Goals</h2>
               <p>Run pipelines that perform OSCAL schema validation, determining the conformance of your OSCAL files to the
            rules defined by an appropriate schema.</p>
               <p>See how these are wired up in XProc, along with a sense of the complexity, tradeoffs and possibilities.</p>
               <p>Gain some sense of wider issues related to schemas, schema evolution, data set regularity and
            predictability.</p>
            </section>
            <section>
               <h2>Prerequisites</h2>
               <p>You have succeeded in prior exercises, including tools installation and setup.</p>
               <p> You know what OSCAL is and <a href="https://pages.nist.gov/OSCAL/resources/concepts/validation/">what
                  <q>validation</q> means</a> in the context of XML and data processing. You are aware of the existence
            of the OSCAL schemas to define syntax for its models in XML and in JSON.</p>
            </section>
            <section>
               <h2>Resources</h2>
               <p>This unit relies on the <a href="../../../projects/oscal-validate/readme.md">oscal-validate project</a> in
            this repository, with its files. Like other projects this one may have installation or setup pipelines to
            run.</p>
               <p>Additionally if you have your own OSCAL to bring, especially OSCAL catalogs, bring them along.</p>
               <p>Learners who are completely new to XML may find themselves in deep, quite soon. Then too, and in any case,
            without a problem of your own to solve, any demonstration here will be very abstract. Counter these problems
            with more resources:</p>
               <ul>
                  <li>Work closely with a partner who can bring the data</li>
                  <li>Reach out via Github Discussions with questions</li>
               </ul>
            </section>
            <section>
               <h2>Step one: validate some OSCAL XML</h2>
               <p>The project contains pipelines that perform validation. Any description that appears here may go out of
            date, so check everything stated in the tutorial coverage against the actual files.</p>
               <section>
                  <h3>Boldly coding</h3>
                  <p>There are those who will look at a screen with a traceback with equanimity and dispassion - and those who
               will recoil. Maybe you are one of the fearless, maybe even one of the curious. Open the pipeline code in
               a programmer's text editor or IDE (integrated development environment) and get a sense of just how
               malleable it is.</p>
                  <p>XML comment syntax takes the form of angle-bracketed text with dashes (hyphens) and a bang: <code>&lt;!â
                  --&gt;</code>. You will see comments throughout the code examples, sometimes as inline documentation, and
               sometimes for entire blocks of inactive code.</p>
                  <p>The intrepid explorer will try both <i>tweaking</i>, around the edges, then <i>adapting</i>, with new
               ideas.</p>
                  <p>The best of it is, as you learn how <b>a traceback is a tasty treat</b>, you enter a virtuous learning
               cycle. Repair a few bugs and you might get a taste for it. (Try the <a href="oscal-validate_102_src.html"
                        class="LessonUnit">102 Lesson unit </a>)</p>
                  <p>If you'd rather keep your feet dry, advance to the next Lesson.</p>
               </section>
            </section>
            <section>
               <h2>Step two: validate and report</h2>
               <p>Schema validation is an automated process, but it is typically done not for its own sake, but rather as a
            preliminary to some other kind of processing, data analysis or manipulation. Accordingly a validator is
            frequently designed to be deployed silently - it only makes noise (emits errors or exceptions) if and as
            problems arise. Silence is good news when it comes to validators.</p>
               <p>However, we also have a step that instead of ending silently, reports summary results.</p>
               <section>
                  <h3>Optional: confirm the validation</h3>
               </section>
            </section>
            <section>
               <h2>Step three: Run a batch validator</h2>
               <img src="ugly-traceback.png"
                    alt="ugly-traceback.png">
               <p>TODO Two ways: separate pipeline; and single pipeline; also a 'switcher' pipeline?</p>
               <p> Also see 102 for a pipeline to digest results of batch validation plus see field testing.</p>
            </section>
            <section>
               <h2>What could possibly go wrong?</h2>
               <p>As always, determining early whether a problem can be solved or mitigated at home, versus when it requires
            some kind of external intervention, is key.</p>
               <p>Fortunately many of the issues that can prevent validation processes from working are the same issues that
            prevent processing in general, and as such are recognizable from the same tracebacks.</p>
               <p>If you do not yet understand the critical distinction between syntactic correctness or
               <q>well-formedness</q>, and validity against the rules stipulated by a model, check out <a href="https://pages.nist.gov/OSCAL/resources/concepts/validation/">the OSCAL web site on that
            topic</a>.</p>
               <p>A syntax error will typically produce a similar or the same error in a validation process as it does in any
            process, inasmuch as a syntax error prevents any kind of processing to be conducted dependably (by a
            deterministic process). As usual in such cases, expect to see console tracebacks and error messages when
            things go wrong (resources are unavailable or out of order), but not process outputs, whether messages or
            files.</p>
               <p>For the same reason, this class of problems is usually found early in pipeline development. Once a pipeline
            is running with tools operating and configured properly, this problem goes away. A developer breaking new
            ground still must be able to recognize them.</p>
               <p>The more interesting class of problems is what are called <q>validation errors</q>, which somewhat
            confusingly represents that class of problems that result not from files that are syntactically erroneous
            and therefore cannot be read, but more subltly, in data instances that can be read, but that when read, are
            found to fail to conform to expectations, as expressed in an applicable rules set.</p>
               <p>Such a rules set is called a <q>schema</q> and deploying a schema to impose regularity and predictability
            over an open-ended body of data â including data that has not yet been created â is a primary responsibility
            of an entity that seeks to define an interoperable language, supporting robust data interchange across
            organizational boundaries.</p>
               <section>
                  <h3>When you know your schema</h3>
               </section>
               <section>
                  <h3>When you must determine a schema</h3>
               </section>
            </section>
            <section>
               <h2>What is this schema?</h2>
               <section>
                  <h3>Schemas and document types</h3>
                  <p>Schemas turn an open-world problem into a closed-world problem, giving it boundaries and helping to make
               it manageable. It traces the outline of the expected. Anything it does not account for can be considered
               out of consideration, other things being equal. As such it is an extremely useful <q>touchstone
                  process</q> (it might be called) that reveal early what kinds of risks there are, if any (and only if
               risks of wasted effort and resources), from processing inputs not fit, defective or <q>to spec</q>. There
               are many ways of designing and building information processing systems with and around schemas, and many
               types of schemas for specific technologies. Even within the relatively narrow context of XML, there are
               many schema validation technologies available, and many approaches to validation using different kinds of
               schemas or schemas in different ways.</p>
                  <p>Even within this complexity, however, a certain regularity is accomplished by the most typical use case
               and application for a schema, broadly, namely to define and assert a <i>document type</i> for an instance
               or recognized entity evaluated by an XML processor. XML documents, that is to say, share many general
               characteristics (for example rules of syntax), while in other respects their differences are critical. In
               order that we can distinguish, for example, between a document as a <b>bill</b> (or invoice) and a
                  <b>receipt</b> (two things that may superficially look somewhat similar), we can distinguish between
               apparently small but critical distinctions that define and characterize them - the rules they follow.</p>
                  <p>OSCAL deploys a range of models, each with its own schemas (for XML and JSON variants of the model). Each
               model describes a different OSCAL type.</p>
               </section>
            </section>
            <section>
               <h2>Critique so far</h2>
               <p>At time of writing this tutorial is very rough, with very much extension and refinement remaining to be
            done.</p>
               <p>Tell us what we are doing right and wrong so far. Help us understand where things need to be explained, or
            better, illuminated by saying less - where is it too thick.</p>
            </section>
         </section>
         <section class="unit"
                  id="oscal-validate_102">
            <h1>102: Validating OSCAL</h1>
            <section>
               <h2>Goals</h2>
               <p>Apply XProc for OSCAL schema validation, to determine the conformance of your OSCAL files to the rules
            defined by an appropriate schema.</p>
               <p>Gain some sense of wider issues related to schemas, schema evolution, data set regularity and
            predictability.</p>
            </section>
            <section>
               <h2>Prerequisites</h2>
               <p>You have succeeded in prior exercises, including tools installation and setup. You know what OSCAL is and
            what <q>validation</q> means in the context of XML and data processing.</p>
            </section>
            <section>
               <h2>Resources</h2>
               <p>This unit relies on the <a href="../../../projects/oscal-validate/readme.md">oscal-validate project</a> in
            this repository, with its files. Like other projects this one may have installation or setup pipelines to
            run.</p>
            </section>
            <section>
               <h2>Schema validation under XProc / code review</h2>
            </section>
            <section>
               <h2>validating sets of documents, handling outputs</h2>
               <p>TODO Two ways: separate pipeline; and single pipeline; also a 'switcher' pipeline?</p>
               <p> Also see 102 for a pipeline to digest results of batch validation plus see field testing.</p>
               <p>[for 102] XProc 3.0 validation steps include an option <code>assert-valid</code> (takes a Boolean
               <code>true</code> or <code>false</code>), which configures how the step works when validation errors are
            encountered. This is essentially a switch between asking the XProc engine to stop with an error when inputs
            presented to the step show validation errors - often the preferred fail-safe solution for validity checks -
            or whether to continue processing the document, so as (for example) to survey it further or treat it
            somehow. Validation errors are reported back to the pipeline in either case - by virtue of validating, we
            will always get a report of where such errors occur. But with assert-valid off (<code>false</code>), if we
            wish we can, instead of quitting, continue with processing.</p>
               <p>One reason to continue with processing is because we are aggregating validation results from a set of
            documents, so we do not want an error if any of them fails. Instead we wish only to validate them and to
            collect the reports.</p>
               <p>See project <a href="../../../projects/schema-field-tests/readme.md">schema-field-tests</a> for deeper
            coverage of validation, batch validation and schema testing.</p>
               <section>
                  <h3>Rule of thumb - <code>assert-valid</code>
                  </h3>
                  <p>Use <code>assert-valid='true'</code> when the purpose of schema validation in a pipeline is as a
               screener, with the results to be processed further as input to a step. This reflects and enforces a
               logical dependency between the schema as a contract (enforceable by the assertion), and downstream
               processing built against the schema's models. It means that downstream processors are relieved thereby of
               an entire class of data processing errors, due to incorrect or inadequate or unforeseen inputs.</p>
                  <p>Use <code>assert-valid='false'</code> when a document is to be discarded after validation, while the
               results of validation - its reports, if any - are taken up. Typically this is done on files in entire
               collections, so halting the process and delivering an error is not in order for bad inputs that are
               detected in any case.</p>
               </section>
            </section>
            <section>
               <h2>Review: why validate?</h2>
               <p>The simplest reason we use schemas to validate documents is that it prevents certain kinds of errors that
            can be difficult to manage otherwise. Schema validation is part of the tool set of <q>jigs and gauges</q> we
            use to keep our models and our data aligned.</p>
               <p>Schemas give us information subject to inferencing, such that when a given schema is applicable, it shows us
            (as designers and engineers) what is and can be known about our data, in general, and what is unknown, where
            are its dimensions of variance. A schema defines a boundary of in / legitimate, and out - illegitimate for
            our purposes and out of scope, however legitimate it may be otherwise. It </p>
            </section>
            <section>
               <h2>102: Defining a pipeline step</h2>
            </section>
            <section>
               <h2>102: Other schemas vs/and other models</h2>
               <p>The main reason it is useful to have all the OSCAL catalog validation in this project use the same step is
            that it consolidates control of which version, indeed which copy and location, of the OSCAL catalog schema
            (XSD) is to be used. Having such a single point of control means we can manage all the tools that use this
            logic together. If we wish to use an XProc validation step directly - hard-wiring a schema into a step -
            remains an option.</p>
            </section>
            <section>
               <h2>102: XProc exception handling</h2>
               <p>other workflows and orchestrations - using schema validation as gateway </p>
               <p>setting schema validation to require-valid=false and using try/catch to trap - as a way of annotating
            instances on the way through (schema-field-tests)</p>
            </section>
            <section>
               <h2>599: some XProc features</h2>
            </section>
         </section>
      </section>
      <section class="lesson"
               id="oscal-publish">
         <section class="unit"
                  id="oscal-publish_101">
            <h1>101: Some OSCAL publishing</h1>
            <section>
               <h2>Goals</h2>
               <p>Use XProc to produce publication artifacts such as HTML (web) pages or Markdown pages from OSCAL. Consider
            how other results and reports are created including PDF and other formats.</p>
               <p>Gain a sense of capabilities and limitations of automated ('lights-out') production of publications from
            OSCAL.</p>
            </section>
            <section>
               <h2>Prerequisites</h2>
               <p>You have succeeded in prior exercises, including tools installation and setup.</p>
               <p>You are acquainted with NIST SP 800-53, rev. 5, the publication that provides the model and requirements set
            for this application, in its <a href="https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf">final PDF
            revision</a>.</p>
            </section>
            <section>
               <h2>Resources</h2>
               <p>This unit relies on the <a href="../../../projects/oscal-publish/readme.md">oscal-publish project</a> in
            this repository, with its files. Like other projects this one may have installation or setup pipelines to
            run.</p>
               <p>Additionally if you have your own OSCAL to <q>print</q>, especially OSCAL catalogs, bring it along.</p>
               <p>
                  <a href="https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf">SP  800-53 rev 5
               (PDF)</a> is an essential reference.</p>
            </section>
            <section>
               <h2>Step one: produce HTML</h2>
            </section>
            <section>
               <h2>Step two: edit and adjust</h2>
            </section>
         </section>
         <section class="unit"
                  id="oscal-publish_102">
            <h1>102: More OSCAL Publishing</h1>
            <section>
               <h2>Goals</h2>
            </section>
            <section>
               <h2>Prerequisites</h2>
               <p>Run the pipelines described in <a href="oscal-publish_101_src.html"
                     class="LessonUnit">the 101 Lesson</a>
               </p>
            </section>
            <section>
               <h2>Resources</h2>
               <p>Like the <a href="oscal-publish_101_src.html"
                     class="LessonUnit">101 lesson</a>, this lesson unit uses the <a href="../../../projects/oscal-publish/readme.md">oscal-import project</a> in this repo.</p>
               <p>Publishing your own OSCAL in these pipelines is also worth trying - the worst they can do is fail. Bring
            OSCAL, especially OSCAL catalogs.</p>
               <p>OSCAL JSON catalogs can be accommodated by splicing together a conversion pipeline with a publishing
            pipeline, as described in the exercise.</p>
            </section>
            <section>
               <h2>Step One: any old OSCAL catalog</h2>
            </section>
            <section>
               <h2>Step Two: publishing OSCAL JSON</h2>
            </section>
         </section>
      </section>
      <section class="lesson"
               id="oscal-produce">
         <section class="unit"
                  id="oscal-produce_101">
            <h1>101: Producing OSCAL from a publication format</h1>
            <section>
               <h2>Goals</h2>
               <p>Learn how high-quality XML can be produced from uncontrolled source data, in this case an HTML file (web
            page) produced by a commodity tool (Adobe Acrobat) from a publicly-available PDF document. The example
            document is US Army Field Manual (FM) 6-22, edition of 2022: <b>Developing Leaders</b>. Its Chapter 4 in
            particular offers a well-structured data set rich in what is called <q>semantics</q> - while that term is
            not here defined - or at least arguably shows a vivid instance of how electronic forms of expression (XML,
            markup) can make it possible to translate semantics, however defined, into applications.</p>
               <p>Observe how a deterministic data processing framework can be tuned or programmed to resolve anomalies in
               <q>bad inputs</q> (but not so bad), producing a well-formatted, cleanly encoded edition at a higher level
            of quality and capability, using a traceable and verifiable process.</p>
               <p>Along the way, learn something about XProc, XSLT transformations, XML, NISO STS format (a standard encoding
            for supporting publication in electronic formats), OSCAL, US Army field manuals and documentation, and the
            focus of FM 6-22: Leadership and leadership development.</p>
            </section>
            <section>
               <h2>Prerequisites</h2>
               <p>You have succeeded in prior exercises, including tools installation and setup.</p>
               <p>You are familiar with NIST SP 800-53, rev. 5, the publication that provides the model and requirements set
            for this application, in its <a href="https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf">final PDF
            revision</a>.</p>
            </section>
            <section>
               <h2>Resources</h2>
               <p>This unit relies on the <a href="../../../projects/oscal-publish/readme.md">oscal-publish project</a> in
            this repository, with its files. Like other projects this one may have installation or setup pipelines to
            run.</p>
               <p>Additionally if you have your own OSCAL to bring, especially OSCAL catalogs, bring them along.</p>
               <p>
                  <a href="https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf">SP  800-53 rev 5
               (PDF)</a> is an essential reference.</p>
            </section>
            <section>
               <h2>Step one: acquire resources</h2>
               <p>As usual, the project contains pipelines to be used to acquire copies of resources, placing them in the
            project lib directory . These may include dependencies for steps in the pipeline (read details in the <a href="oscal-produce_102_src.html"
                     class="LessonUnit">102 lesson</a>) so it is a good idea to run them all.</p>
               <p>These include a pipeline that copies the PDF original from which the HTML input (next step) was derived,
            using a COTS (commercial off-the-shelf) tool. Inspect and read this PDF to see what the upstream data looks
            like - and get a sense of what the data set looks like when the structures are <q>painted</q> on the
            page.</p>
               <p>The extraction and mapping focuses on Chapter 4 only of this document. Learn more about FM 6-22,
               <i>Developing Leaders</i> in <a href="../../../projects/oscal-import/readme.md">the project
            readme</a>.</p>
            </section>
            <section>
               <h2>Step two: examine HTML source data</h2>
               <p>For this exercise, we have already produced HTML from the PDF source: find it cached as an <a href="../../../projects/oscal-import/source/export/fm6_22.html">input file in the project folder</a>.
            Next to it is <a href="../../../projects/oscal-import/source/export/fm6_22_e.html">a copy with whitespace
               added for legibility</a>, which may be easier to inspect.</p>
               <p>Compare this to the PDF document you have downloaded (or the document on line). Open the HTML file in a web
            browser to see what it looks like in display, but also inspect its source code. You don't have to read it:
            just seeing it is enough.</p>
               <section>
                  <h3>Warning: fragile!</h3>
                  <p>
                     <i>Do not delete, replace, rename or alter this file</i>, as all the subsequent processing is tuned
               specifically to it, its internal organization, regularities and anomalies. Another process input - even
               an HTML file showing the same text (to any definition of <q>same</q>) - is highly unlikely to deliver
               acceptable results from the pipeline. This is an important factor to keep in mind when considering the
               scalability of the operation demonstrated. Producing a clean and legible <i>encoding</i> for the document
               does not require less expertise than producing the original; in some ways (for example with respect to
               knowledge of text encoding formats) it requires more.</p>
                  <p>For some data sets, however, even a bespoke transformation pipeline such as this can be profitable, even
               beyond its impacts in greater availability and utility of the information. The expense and effort may be
               warranted when the source document is of such high quality and consistency instrinsically, that exposing
               its <q>native semantics</q> via a rational and explicable text encoding - a standards-based, platform-
               and application-independent encoding - such as OSCAL - becomes attractive for the specific kinds of
               affordances â the leverage â it gives for processing the information. In FM 6-22 Chapter 4, we have
               exactly such a document, which fits well into standards-based documentary formats such as NISO STS and,
               eventually, OSCAL.</p>
                  <p>Because PDF is relatively opaque, extracting data from a PDF file with format intact is not a trivial
               exercise. PDF is arguably not a <q>data format</q> at all, but a language used to control a device for
               casting pixels and glyphs onto a page or screen. This being its purpose, no tool can do more â at least
               without the benefit of an external information source â than create outputs that <i>appear</i> to present
               the document with its organization and information intact, if rendered on screen with an appropriate tool
               (such as, in the case of an HTML rendition of the page, a web browser). In so doing it may meet a low
               baseline of functional requirement - the document is legible - while failing to meet others. In
               particular: failing to meet requirements for clean and well-structured data fit for reuse in a broad
               range of scenarios and applications, not only page production.</p>
                  <p>Indeed this HTML file makes an excellent example of the kind of rather-poor encoding produced by a
               market-leading HTML export utility. Experts in data conversion might find other ways to make better
               encoding from the PDF - a task out of scope for us. (As we are not trying to find the best export
               utility, but on the contrary to demonstrate methods available so we do not have to do so.) This HTML is
               good enough, and that is what is often available.</p>
               </section>
            </section>
            <section>
               <h2>Step three: run the pipeline</h2>
               <p>The pipeline starts by loading <a href="../../../projects/oscal-import/source/export/fm6_22.html">source/export/fm6_22.html</a>
               </p>
               <p>It produces output files in a <code>temp</code> directory. If you want to save out any of these outputs, you
            can copy files out, or edit the pipeline.</p>
               <p>Two files are created, both representing the Field Manual, each in a different format:</p>
               <ul>
                  <li>NISO STS (Standards Tag Suite) format describes the full text, with tables, in XML</li>
                  <li>OSCAL (Open Security Controls Assessment Language) factors out a richer <q>semantic</q> view of the data
               in OSCAL format, again presenting the full text of Chapter 4, but this time with externalized control
               sets, representing 25 <b>attributes</b> and 50 <b>competencies</b> as these are described in Field Manual
               6-22 and its related documents such as ADRP 6-22 (describing the US Army Leadership Requirements
               Model).</li>
               </ul>
            </section>
            <section>
               <h2>Step four (optional): run the pipeline in diagnostic mode</h2>
            </section>
         </section>
         <section class="unit"
                  id="oscal-produce_102">
            <h1>102: Producing OSCAL from uncooked data</h1>
            <section>
               <h2>Goals</h2>
               <p>Learn about the internals of an XML-based data extraction and mapping process, an <q>uphill data
               conversion</q>.</p>
               <p>Get a chance to see how XSLT gives XProc a way to address conversion at appropriate levels of scale and
            abstraction.</p>
               <p>See an example of how an XProc pipeline can integrate validation to provide runtime quality-assurance and
            regression testing.</p>
            </section>
            <section>
               <h2>Prerequisites</h2>
               <p>Run the pipelines described in <a href="oscal-produce_101_src.html"
                     class="LessonUnit">the 101 Lesson</a>
               </p>
            </section>
            <section>
               <h2>Resources</h2>
               <p>Like the <a href="oscal-produce_101_src.html"
                     class="LessonUnit">101 lesson</a>, this lesson unit uses the <a href="../../../projects/oscal-import/readme.md">oscal-import project</a> in this repo.</p>
            </section>
            <section>
               <h2>Step one: run the pipeline in diagnostic mode</h2>
            </section>
            <section>
               <h2>Step two: survey the pipeline steps</h2>
               <section>
                  <h3>feature: step by step <q>up the hill</q>
                  </h3>
                  <p>XProc is well suited for supporting an incremental development process based on an analysis/improvement
               loop. One at a time, we isolate operations to perform on the incoming data to refashion it - renaming and
               sometimes restructuring - into a more clearly encoded representation, closer to the goal of valid and
               high-quality OSCAL.</p>
                  <p>Keeping the operations isolated in separate transformations has important opportunities. We can</p>
                  <ul>
                     <li>Save out interim representations for inspection and validation (formal or informal)</li>
                     <li>Isolate sub-processes for specialized requirements (micro-structures)</li>
                     <li>Produce and persist (save out) any useful interim representation as a process by-product valuable in
                  its own right</li>
                  </ul>
                  <p>As it happens, the document example here is easier to convert into OSCAL if we convert into NISO STS
               format first. This gives us a good separation of concerns between producing any adequate semantic
               representation (in principle, irrespective of vocabulary) and producing a final and optimized OSCAL
               representation. Using NISO STS saves our having to invent a <q>bespoke</q> interim vocabulary, or use
               HTML, for this purpose, while introducing rigor. So our pipeline has two main parts:</p>
                  <ul>
                     <li>Convert raw text into running NISO STS format (clean up, fix up, mapping)</li>
                     <li>Convert NISO STS into OSCAL (refactoring)</li>
                  </ul>
                  <p>And when we map it out in detail:</p>
                  <ul>
                     <li>Convert HTML into NISO STS</li>
                     <li>Save (valid or invalid)</li>
                     <li>Validate against STS schema (QA check)</li>
                     <li>Convert STS into OSCAL</li>
                     <li>Save (valid or invalid)</li>
                     <li>Validate against OSCAL schema and rules (QA checks)</li>
                     <li>Report all validation results</li>
                  </ul>
                  <p>Thus we can expect our runtime to deliver three outputs:</p>
                  <ul>
                     <li>An XML file representing the document, nominally in NISO STS format</li>
                     <li>An XML file representing the document, nominally in OSCAL format</li>
                     <li>Validation results in the console: <q>All clear</q> or <q>Uhoh, please check</q> if validation errors
                  were reported</li>
                  </ul>
                  <p>Specific validation errors are not, however, reported, only the summary finding. To see validation
               errors, run the files with a validator or pipeline that reports them (see projects <a href="../../../projects/oscal-validate/readme.md">oscal-validate project</a>
                     <a href="../../../projects/schema-field-tests/readme.md">oscal</a> ). Depending on the validation
               technology being used - XML Schema (XSD), RelaxNG, Schematron or other - this can be done in a variety of
               ways using commodity tools.</p>
                  <p>As Step One also shows, the end-to-end process is not only robust (providing its own QA checks), it is
               also traceable, replicable (on the same inputs) and adaptable (for similar inputs), as it captures and
               codifies what an analyst learns about the incoming data, making this knowledge accessible and
               reusable.</p>
                  <p>Having a valid NISO STS instance as a <q>side effect</q> of this process also means we are saved the work
               of mapping it back down from OSCAL into STS, if for any reason an STS version is wanted.</p>
               </section>
               <section>
                  <h3>feature: saving intermediate files conditionally</h3>
               </section>
               <section>
                  <h3>feature: inline and out-of-line XSLT transformations</h3>
               </section>
               <section>
                  <h3>feature: validations on the fly</h3>
                  <p>As described, the pipeline also validates its outputs on the fly and reports summary findings for these
               processes. Summary findings are judged to be enough since the particular validation errors are not always
               actionable, while when they are, they are also easy to determine by other means. (Validate the file
               implicated using a different pipeline or tool.)</p>
                  <p>In this implementation, these checks are provided fail-safes in case a user has failed to run pipelines
               to acquire schemas required for validation. Missing one of these resources results not in an error or
               process failure, but a warning that validation has not been performed.</p>
                  <p>Alternatively to downloading and securing these dependeny resource, a user always has the option of
               rewriting the pipeline to use a different resource or do something altogether different.</p>
               </section>
            </section>
            <section>
               <h2>Step 2.5: Inspect the STS version</h2>
               <p>This repository is devoted to OSCAL, but inasmuch as OSCAL comes in an XML format, it also plays well with
            other XML-based formats such as DITA, TEI or NISO STS (a member of the NISO JATS family). In this
            application, NISO STS or NISO BITS both offer credible alternatives for representing the document in
            machine-readable form. Moreover - what is most interesting and important - such an encoding (we have chosen
            STS as it is used elsewhere in our agency) is somewhat easier to produce than OSCAL. This is because OSCAL,
            while very flexible in some respects, provides rigor in other respects especially with regard to document
            structures: it demands and rewards regularity, not structural variation, among its parts. Field Manual 6-22
            Chapter 4 is interesting in that it presents such regularity, but only implicitly by way of formatting
            conventions. STS can represent these conventional forms as given, without refactoring.</p>
               <p>This difference is easily appreciated by comparing the two variants.</p>
               <p>To display an STS document in a browser for reference or proofreading, an STS application such as the <a href="https://pages.nist.gov/xslt-blender/sts-viewer/">NIST/ITL/CSD STS Viewer</a> can be useful.</p>
            </section>
            <section>
               <h2>Step three: break and repair</h2>
            </section>
            <section>
               <h2>Step four: research XSLT</h2>
            </section>
         </section>
      </section>
   </body>
</html>