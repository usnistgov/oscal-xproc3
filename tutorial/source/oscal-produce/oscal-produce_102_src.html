<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <title>102: Producing OSCAL from uncooked data</title>
   </head>
   <body>
      <h1>102: Producing OSCAL from uncooked data</h1>
      <section>
         <h2>Goals</h2>
         <p>Learn about the internals of an XML-based data extraction and mapping process, an <q>uphill data
               conversion</q>.</p>
         <p>Get a chance to see how XSLT gives XProc a way to address conversion at appropriate levels of scale and
            abstraction.</p>
         <p>See an example of how an XProc pipeline can integrate validation to provide runtime quality-assurance and
            regression testing.</p>
      </section>
      <section>
         <h2>Prerequisites</h2>
         <p>Run the pipelines described in <a href="oscal-produce_101_src.html">the 101 Lesson</a></p>
      </section>
      <section>
         <h2>Resources</h2>
         <p>Like the <a href="oscal-produce_101_src.html">101 lesson</a>, this lesson unit uses the <a
               href="../../../projects/oscal-from-pdf/readme.md">oscal-from-pdf project</a> in this repo.</p>
      </section>
      <section>
         <h2>Step one: run the pipeline in diagnostic mode</h2>
      </section>
      <section>
         <h2>Step two: survey the pipeline steps</h2>
         <section>
            <h3>feature: step by step <q>up the hill</q></h3>
            <p>XProc is well suited for supporting an incremental development process based on an analysis/improvement
               loop. One at a time, we isolate operations to perform on the incoming data to refashion it - renaming and
               sometimes restructuring - into a more clearly encoded representation, closer to the goal of valid and
               high-quality OSCAL.</p>
            <p>Keeping the operations isolated in separate transformations has important opportunities. We can</p>
            <ul>
               <li>Save out interim representations for inspection and validation (formal or informal)</li>
               <li>Isolate sub-processes for specialized requirements (micro-structures)</li>
               <li>Produce and persist (save out) any useful interim representation as a process by-product valuable in
                  its own right</li>
            </ul>
            <p>As it happens, the document example here is easier to convert into OSCAL if we convert into NISO STS
               format first. This gives us a good separation of concerns between producing any adequate semantic
               representation (in principle, irrespective of vocabulary) and producing a final and optimized OSCAL
               representation. Using NISO STS saves our having to invent a <q>bespoke</q> interim vocabulary, or use
               HTML, for this purpose, while introducing rigor. So our pipeline has two main parts:</p>
            <ul>
               <li>Convert raw text into running NISO STS format (clean up, fix up, mapping)</li>
               <li>Convert NISO STS into OSCAL (refactoring)</li>
            </ul>
            <p>And when we map it out in detail:</p>
            <ul>
               <li>Convert HTML into NISO STS</li>
               <li>Save (valid or invalid)</li>
               <li>Validate against STS schema (QA check)</li>
               <li>Convert STS into OSCAL</li>
               <li>Save (valid or invalid)</li>
               <li>Validate against OSCAL schema and rules (QA checks)</li>
               <li>Report all validation results</li>
            </ul>
            <p>Thus we can expect our runtime to deliver three outputs:</p>
            <ul>
               <li>An XML file representing the document, nominally in NISO STS format</li>
               <li>An XML file representing the document, nominally in OSCAL format</li>
               <li>Validation results in the console: <q>All clear</q> or <q>Uhoh, please check</q> if validation errors
                  were reported</li>
            </ul>
            <p>Specific validation errors are not, however, reported, only the summary finding. To see validation
               errors, run the files with a validator or pipeline that reports them (see projects <a
                  href="../../../projects/oscal-validate/readme.md">oscal-validate project</a>
               <a href="../../../projects/schema-field-tests/readme.md">oscal</a> ). Depending on the validation
               technology being used - XML Schema (XSD), RelaxNG, Schematron or other - this can be done in a variety of
               ways using commodity tools.</p>
            <p>As Step One also shows, the end-to-end process is not only robust (providing its own QA checks), it is
               also traceable, replicable (on the same inputs) and adaptable (for similar inputs), as it captures and
               codifies what an analyst learns about the incoming data, making this knowledge accessible and
               reusable.</p>
            <p>Having a valid NISO STS instance as a <q>side effect</q> of this process also means we are saved the work
               of mapping it back down from OSCAL into STS, if for any reason an STS version is wanted.</p>
         </section>
         <section>
            <h3>feature: saving intermediate files conditionally</h3>
         </section>
         <section>
            <h3>feature: inline and out-of-line XSLT transformations</h3>
         </section>
         <section>
            <h3>feature: validations on the fly</h3>
            <p>As described, the pipeline also validates its outputs on the fly and reports summary findings for these
               processes. Summary findings are judged to be enough since the particular validation errors are not always
               actionable, while when they are, they are also easy to determine by other means. (Validate the file
               implicated using a different pipeline or tool.)</p>
            <p>In this implementation, these checks are provided fail-safes in case a user has failed to run pipelines
               to acquire schemas required for validation. Missing one of these resources results not in an error or
               process failure, but a warning that validation has not been performed.</p>
            <p>Alternatively to downloading and securing these dependeny resource, a user always has the option of
               rewriting the pipeline to use a different resource or do something altogether different.</p>
         </section>
      </section>
      <section>
         <h2>Step 2.5: Inspect the STS version</h2>
         <p>This repository is devoted to OSCAL, but inasmuch as OSCAL comes in an XML format, it also plays well with
            other XML-based formats such as DITA, TEI or NISO STS (a member of the NISO JATS family). In this
            application, NISO STS or NISO BITS both offer credible alternatives for representing the document in
            machine-readable form. Moreover - what is most interesting and important - such an encoding (we have chosen
            STS as it is used elsewhere in our agency) is somewhat easier to produce than OSCAL. This is because OSCAL,
            while very flexible in some respects, provides rigor in other respects especially with regard to document
            structures: it demands and rewards regularity, not structural variation, among its parts. Field Manual 6-22
            Chapter 4 is interesting in that it presents such regularity, but only implicitly by way of formatting
            conventions. STS can represent these conventional forms as given, without refactoring.</p>
         <p>This difference is easily appreciated by comparing the two variants.</p>
         <p>To display an STS document in a browser for reference or proofreading, an STS application such as the <a
               href="https://pages.nist.gov/xslt-blender/sts-viewer/">NIST/ITL/CSD STS Viewer</a> can be useful.</p>
      </section>
      <section>
         <h2>Step three: break and repair</h2>
      </section>
      <section>
         <h2>Step four: research XSLT</h2>
      </section>
   </body>
</html>