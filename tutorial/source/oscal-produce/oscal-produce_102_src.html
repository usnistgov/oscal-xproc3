<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <title>102: Producing OSCAL from crude data</title>
      <meta charset="utf-8" />
   </head>
   <body data-track="maker">
      <h1>102: Producing OSCAL from crude data</h1>
      <section>
         <h2>Goals</h2>
         <p>Learn about the internals of XML-based data extraction and mapping processes, what we call an <q>uphill</q>
            data conversion, enhancement or refinement.</p>
         <p>Get a chance to see how XSLT and other steps together give XProc ways to address conversion problems at
            appropriate levels of scale and abstraction, both for general and for specific or narrow cases.</p>
         <p>See examples of how XProc pipelines can integrate validation processes to provide runtime quality-assurance
            and regression testing.</p>
      </section>
      <section>
         <h2>Prerequisites</h2>
         <p>You are familiar with the idea and probably the practice of running XProc pipelines, as described in earlier
            lessons.</p>
         <p>You have at least glanced at the <a href="oscal-produce_101_src.html" class="LessonUnit">preceding lesson
               unit</a> for an explanation of the examples discussed here.</p>
      </section>
      <section>
         <h2>Resources</h2>
         <p>Like the preceding lesson unit, this one relies on XProc examples in this repository:</p>
         <ul>
            <li>USDS Playbook – 13 OSCAL controls (high level) derived from a simple web page produced by the US Digital
               Service</li>
            <li>NIST SP 800-171 from NIST Computer Cybersecurity and Privacy Reference Tool (CPRT) JSON</li>
            <li>US Army Field Manual 6-22 - via HTML from PDF source, with NISO STS format as an additional result</li>
         </ul>
         <p><i>Caution: all produced versions of these public documents are non-normative, unauthorized (lawful within
               terms of reuse in the public domain) and not for publication, being produced only for demonstration of
               tools and capabilities.</i></p>
      </section>
      <section>
         <h2>Interactive debugging in XProc</h2>
         <p>XXX picking up from earlier …?</p>
      </section>
      <section>
         <h2>How to test results</h2>
         <p>Both automated (replicable) and non-automated (irreplicable) test methodologies can be applied.</p>
      </section>
      <section></section>
      <section>
         <h2>XProc so far: a summary</h2>
         <p>XXX Survey these pipelines for XProc elements used so far.</p>
      </section>
      <section>
         <h2>Data enhancement as converting <q>up</q>: the metaphor</h2>
         <p>Process of cleaning, refining, restructuring and enhancing.</p>
         <p>Typically proceeds as a series of waypoints between sequences of operations devoted to a single task. These
            waypoints can be saved out for inspection and analysis, and validated using any and all means available:
            schemas; transformations; simple queries or path expressions (XPath) or by other means.</p>
         <section>
            <h3>feature: step by step <q>up the hill</q></h3>
            <p>XProc is well suited for supporting an incremental development process based on an analysis/improvement
               loop. One at a time, we isolate operations to perform on the incoming data to refashion it - renaming and
               sometimes restructuring - into a more clearly encoded representation, closer to the goal of valid and
               high-quality OSCAL.</p>
            <p>Keeping the operations isolated in separate transformations has important opportunities. We can:</p>
            <ul>
               <li>Save out interim representations for inspection and validation (formal or informal)</li>
               <li>Isolate sub-processes for specialized requirements (micro-structures)</li>
               <li>Produce and persist (save out) any useful interim representation as a process by-product valuable in
                  its own right</li>
            </ul>
            <p>As it happens, the document example here is easier to convert into OSCAL if we convert into NISO STS
               format first. This gives us a good separation of concerns between producing any adequate semantic
               representation (in principle, irrespective of vocabulary) and producing a final and optimized OSCAL
               representation. Using NISO STS saves our having to invent a <q>bespoke</q> interim vocabulary, or use
               HTML, for this purpose, while introducing rigor. So our pipeline has two main parts:</p>
            <ul>
               <li>Convert raw text into running NISO STS format (clean up, fix up, mapping)</li>
               <li>Convert NISO STS into OSCAL (refactoring)</li>
            </ul>
            <p>And when we map it out in detail:</p>
            <ul>
               <li>Convert HTML into NISO STS</li>
               <li>Save (valid or invalid)</li>
               <li>Validate against STS schema (QA check)</li>
               <li>Convert STS into OSCAL</li>
               <li>Save (valid or invalid)</li>
               <li>Validate against OSCAL schema and rules (QA checks)</li>
               <li>Report all validation results</li>
            </ul>
            <p>Thus we can expect our runtime to deliver three outputs:</p>
            <ul>
               <li>An XML file representing the document, nominally in NISO STS format</li>
               <li>An XML file representing the document, nominally in OSCAL format</li>
               <li>Validation results in the console: <q>All clear</q> or <q>Uhoh, please check</q> if validation errors
                  were reported</li>
            </ul>
            <p>Specific validation errors are not, however, reported, only the summary finding. To see validation
               errors, run the files with a validator or pipeline that reports them (see projects <a
                  href="../../../projects/oscal-validate/readme.md">oscal-validate project</a>
               <a href="../../../projects/schema-field-tests/readme.md">oscal</a> ). Depending on the validation
               technology being used - XML Schema (XSD), RelaxNG, Schematron or other - this can be done in a variety of
               ways using commodity tools.</p>
            <p>As Step One also shows, the end-to-end process is not only robust (providing its own QA checks), it is
               also traceable, replicable (on the same inputs) and adaptable (for similar inputs), as it captures and
               codifies what an analyst learns about the incoming data, making this knowledge accessible and
               reusable.</p>
            <p>Having a valid NISO STS instance as a <q>side effect</q> of this process also means we are saved the work
               of mapping it back down from OSCAL into STS, if for any reason an STS version is wanted.</p>
         </section>
         <section>
            <h3>feature: saving intermediate files conditionally</h3>
         </section>
         <section>
            <h3>feature: inline and out-of-line XSLT transformations</h3>
         </section>
         <section>
            <h3>feature: validations on the fly</h3>
            <p>XXX As described, the pipeline also validates its outputs on the fly and reports summary findings for
               these processes. Summary findings are judged to be enough since the particular validation errors are not
               always actionable, while when they are, they are also easy to determine by other means. (Validate the
               file implicated using a different pipeline or tool.)</p>
            <p>In this implementation, these checks are provided fail-safes in case a user has failed to run pipelines
               to acquire schemas required for validation. Missing one of these resources results not in an error or
               process failure, but a warning that validation has not been performed.</p>
            <p>Alternatively to downloading and securing these dependeny resource, a user always has the option of
               rewriting the pipeline to use a different resource or do something altogether different.</p>
         </section>
         <section>
            <h3>feature: transparency for one-off processes</h3>
         </section>
      </section>
      <section>
         <h2>Step 2.5: Inspect [the STS version] intermediate representations</h2>
         <p>XXX This repository is devoted to OSCAL, but inasmuch as OSCAL comes in an XML format, it also plays well
            with other XML-based formats such as DITA, TEI or NISO STS (a member of the NISO JATS family). In this
            application, NISO STS or NISO BITS both offer credible alternatives for representing the document in
            machine-readable form. Moreover - what is most interesting and important - such an encoding (we have chosen
            STS as it is used elsewhere in our agency) is somewhat easier to produce than OSCAL. This is because OSCAL,
            while very flexible in some respects, provides rigor in other respects especially with regard to document
            structures: it demands and rewards regularity, not structural variation, among its parts. Field Manual 6-22
            Chapter 4 is interesting in that it presents such regularity, but only implicitly by way of formatting
            conventions. STS can represent these conventional forms as given, without refactoring.</p>
         <p>This difference is easily appreciated by comparing the two variants.</p>
         <p>To display an STS document in a browser for reference or proofreading, an STS application such as the <a
               href="https://pages.nist.gov/xslt-blender/sts-viewer/">NIST/ITL/CSD STS Viewer</a> can be useful.</p>
      </section>
      <section>
         <h2>Step three: break and repair</h2>
      </section>
      <section>
         <h2>Step four: for further research</h2>
      </section>
   </body>
</html>