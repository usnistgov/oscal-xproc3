<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <title>101: OSCAL from XML to JSON and back</title>
      <meta charset="utf-8" />
   </head>
   <body data-track="observer">
      <h1>101: OSCAL from XML to JSON and back</h1>
      <section>
         <h2>Goals</h2>
         <p>Learn how OSCAL data can be converted between JSON and XML formats, using XProc.</p>
         <p>Learn something about potential problems and limitations when doing this, and about how to detect, avoid,
            prevent or mitigate them.</p>
         <p>Become familiar with the idea of generic conversions between syntaxes such as XML and JSON (not always
            possible), versus conversions designed to handle a single class or type of documents, such as OSCAL format
            conversions.</p>
      </section>
      <section>
         <h2>Prerequisites</h2>
         <p>You have succeeded in prior exercises, including tools installation and setup.</p>
      </section>
      <section>
         <h2>Resources</h2>
         <p>This unit relies on the <a href="../../../projects/oscal-convert/readme.md">oscal-convert project</a> in
            this repository, with its files. Like all projects in the repo, it aims to be reasonably self-contained and
            self-explanatory. Use your search engine and XProc resources to learn background and terminology.</p>
         <p>Also like other projects, there are preliminaries for acquiring resources, along with pipelines to run.</p>
      </section>
      <section>
         <h2>Step zero: an identity pipeline</h2>
         <p>To verify syntactic correctness (well-formedness) - does it parse?</p>
         <p>To transcode a file from one encoding to another</p>
      </section>
      <section>
         <h2>Step zero-point-five: XML to JSON and back</h2>
         <p>XML and JSON are both <i>data serializations</i>, a term that designates how each of them is to be
            considered and treated – irrespective of any questions of information storage – as a <i>sequence of
               characters</i>. This is a very important commonality, which makes it possible to bring them together in a
            single processing environment such as XProc.</p>
         <p>Along with <i>plain text</i>, perhaps the most important data serialization or <q>format</q> as we call
            them, of the three.</p>
         <p>A simple XProc pipeline can be used to demonstrate this. While doing so, it shows also while this is not as
            simple a process as it seems. Merely to convert from format to format is not enough.</p>
      </section>
      <section>
         <h2>Step one: convert some OSCAL XML into OSCAL JSON</h2>
         <p><a href="../../../projects/oscal-convert/GRAB-RESOURCES.xpl">An acquisition pipeline</a> in the project
            folder collects some OSCAL onto the local system, where it can be managed, easily inspected, controlled, and
            edited if necessary.</p>
         <p>TBD / this all incoherent so far</p>
         <section>
            <h3>The playing field is the internet</h3>
            <p>Keep in mind that XProc in theory, and your XProc engine in practice, may read its inputs using whatever
               protocols it supports, while the <code>file</code> and <code>http</code> protocols are required for
               conformance, and work as they do on the Worldwide Web.</p>
            <p>Of course, permissions must be in place to read files from system locations, or save files to them.</p>
            <p>But when authentication is configured or resources are openly available, using <code>http</code> to reach
               resources or sources can be a very convenient option.</p>
         </section>
         <section>
            <h3>Consider the options</h3>
            <p>TBD - TODO - question - how many and of what sort of source data files - so far there is only the cat
               catalog</p>
            <ul>
               <li>Converting local XML to JSON with a local XSLT</li>
               <li>Converting local data using a remote XSLT</li>
               <li>Remote data with a local XSLT, writing locally - you could try <a
                     href="https://github.com/GSA/fedramp-automation/blob/master/dist/content/rev5/baselines/xml/FedRAMP_rev5_LOW-baseline-resolved-profile_catalog.xml"
                     >https://github.com/GSA/fedramp-automation/blob/master/dist/content/rev5/baselines/xml/FedRAMP_rev5_LOW-baseline-resolved-profile_catalog.xml</a></li>
            </ul>
         </section>
      </section>
      <section>
         <h2>Step two: return trip</h2>
         <p>Two ways: separate pipeline; and single pipeline; also a 'switcher' pipeline?</p>
      </section>
      <section>
         <h2>What is this XSLT?</h2>
         <p>If your criticism of XProc so far is that it makes it look easy when it isn't, you have a point.</p>
         <p>Conversion from XML to JSON isn't free, assuming it works at all.</p>
         <p>In this case, the heavy lifting is done by the XSLT component - the Saxon engine invoked by the
               <code>p:xslt</code> step, applying logic defined in an XSLT stylesheet (aka transformation) stored
            elsewhere. It happens that a converter for OSCAL data is available in XSLT, so rather than having to
            confront this considerable problem ourselves, we drop in the solution we have at hand.</p>
         <p>In later units we will see how using the XProc steps described, rudimentary data manipulations can be done
            using XProc by itself, without entailing the use of either XSLT or XQuery (another capability invoked with a
            different step).</p>
         <p>At the same time, while pipelines are based on the idea of passing data through a series of processes, there
            are many cases where logic is sufficiently complex that it becomes essential to maintain – and test – that
            logic externally from the XProc. At what point it becomes more efficient to encapsulate logic separately
            (whether by XSLT, XQuery or other means), depends very much on the case.</p>
         <p>The <code>p:xslt</code> pipeline step in particular is so important for real-world uses of XProc that it is
            introduced early, to show such a black-box application.</p>
         <p>XProc also makes a fine environment for testing XSLT developed or acquired to handle specific tasks, a topic
            covered in more depth later.</p>
         <p>Indeed XSLT and XQuery being, like XProc itself, declarative languages, it makes sense to factor them out
            while maintaining easy access and transparency for analysis and auditing purposes.</p>
      </section>
      <section>
         <h2>What could possibly go wrong?</h2>
         <p>When coping with errors, syntax errors are relatively easy. But anomalous inputs, especially invalid inputs,
            can result in lost data. (A common reason data is not valid even when it appears to be is that it has
            foreign unknown contents, or contents out of place - the kinds of things that might fail to be converted.)
            The most important concern when engineering a pipeline is to see to it that no data quality problems are
            introduced inadvertantly. While in comparison to syntax or configuration problems, data quality issues can
            be subtle, there is also good news: the very same tools we use to process inputs into outputs, can also be
            used to test and validate data to both applicable standards and local rules.</p>
         <p>Generally speaking, OSCAL maintains <q>validation parity</q> between its XML and JSON formats with respect
            to their schemas. That is to say, the XSD (XML schema) covers essentially the same set of rules for OSCAL
            XML data as the JSON Schema does for OSCAL JSON data, accounting for differences between the two notations,
            the data models and how information is mapped into them. A consequence of this is that valid OSCAL data,
            either XML or JSON, can reliably be converted to valid data in the other notation, while invalid data may
            not be converted at all, resulting in gaps or empty results.</p>
         <p>For this and related reasons on open systems, the working principle in XML is often to formalize a model
            (typically by writing and deploying a schema) as early as possible - or adopt a model already built - as a
            way to institute and enforce schema validation as a <b>prerequisite</b> and <b>primary requirement</b> for
            working with any data set. Validation against schemas is covered in a subsequent lesson unit (coming soon
            near you).</p>
         <section>
            <h3>Intercepting errors</h3>
            <p>One way to manage the problem of ensuring input quality is to validate on the way in, either as a
               dependent (prerequisite) process, or built into a pipeline. Whatever you want to do with invalid inputs,
               including ignoring them and producing warnings or runtime exceptions, can be defined in a pipeline much
               like anything else.</p>
            <p>In the <a href="../../../projects/oscal-publish/publish-oscal-catalog.xpl">publishing demonstration
                  project folder</a> is an XProc that valides XML against an OSCAL schema, before formatting it. The
               same could be done for an XProc that converts the data into JSON - either or both before or after
               conversion.</p>
            <p>Learn more about recognizing and dealing with errors in <a href="oscal-convert_102_src.html"
                  class="LessonUnit">Lesson 102</a>, or continue on to the next project, oscal-validate, for more on
               validation of documents and sets of documents.</p>
         </section>
      </section>
   </body>
</html>