<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <title>101: Converting OSCAL – XML to JSON and JSON to XML</title>
      <meta charset="utf-8" />
   </head>
   <body data-track="observer">
      <h1>101: Converting OSCAL – XML to JSON and JSON to XML</h1>
      <section>
         <h2>Goals</h2>
         <p>Learn how OSCAL data can be converted between JSON and XML formats, using XProc.</p>
         <p>Learn something about potential problems and limitations when doing this, and about how they can be detected
            and avoided, prevented or mitigated.</p>
         <p>Become familiar with the idea of generic conversions between syntaxes such as XML and JSON, versus
            conversions capable of handling a single class or type of documents, such as OSCAL, which offers a fully
            defined mapping supporting lossless, error-free translation across syntaxes.</p>
      </section>
      <section>
         <h2>Prerequisites</h2>
         <p>Having succeeded in prior exercises, including tools installation and setup, you are ready to run
            pipelines.</p>
      </section>
      <section>
         <h2>Resources</h2>
         <p>This unit relies on the <a href="../../../projects/oscal-convert/readme.md">oscal-convert project</a> in
            this repository, with its files. Like all projects in the repo, it aims to be reasonably self-contained and
            self-explanatory. The pipelines there (described below) provide rudimentary support for data conversions,
            demonstrating simplicity and scalability.</p>
      </section>
      <section>
         <h2>Pipeline rundown</h2>
         <p>The XProc pipelines do not do much, as they are wired up to work with minimalistic demonstration data sets.
            This keeps them plain, simple and obvious in exposition, while at the same time it should be apparent how
            they can be readily reconfigured or customized to work on any suitable inputs that may be provided.</p>
         <p>The idea here is simple: run the pipeline and observe its behaviors, including not only the messages it
            emits to the console window, but also the file outputs it generates. Each pipeline should be provided with
            inline comments describing its workings. Also see the <a href="oscal-convert_102_src.html"
               class="LessonUnit">next lesson unit</a> for more details.</p>
         <section>
            <h3><a href="../../../projects/oscal-convert/GRAB-RESOURCES.xpl">GRAB-RESOURCES</a></h3>
            <p>Like other pipelines with this name, run this to acquire resources. In this case, XSLTs used by other
               XProc steps are downloaded from the OSCAL release page.</p>
         </section>
         <section>
            <h3><a href="../../../projects/oscal-convert/BATCH_JSON-TO-XML.xpl">BATCH-JSON-TO-XML</a></h3>
            <p>This pipeline uses an XProc input port to include a set of JSON documents, which it translates into XML
               using <a href="https://www.w3.org/TR/xpath-functions-31/#json-to-xml-mapping">generic semantics defined
                  in XPath</a>. For each JSON file input, an equivalent XML file with the same filename base is
               produced, in the same place.</p>
            <p>As posted, the pipeline reads some minimalistic fictional data, which can be found in the system at the
               designated paths.</p>
            <p>It then uses a pipeline step defined in an imported pipeline, which casts the data and stores the result
               for each JSON file input. In the XProc source, the imported step can be easily identified by its
               namespace prefix, different from the prefix given to XProc elements, as designated by the pipeline's
               developer or sponsor.</p>
            <p>Follow the <code>p:import</code> link (via its <code>href</code> file reference) to find the step that is
               imported. An imported step is invoked by using its <code>type</code> name, given at the top of the XML
               (as is described in more depth <a href="oscal-convert_201_src.html" class="LessonUnit">in a subsequent
                  lesson unit</a>).</p>
         </section>
         <section>
            <h3><a href="../../../projects/oscal-convert/BATCH_XML-TO-JSON.xpl">BATCH-XML-TO-JSON</a></h3>
            <p>This pipeline performs the reverse of the JSON-to-XML batch pipeline. It loads XML files and converts
               them into JSON.</p>
            <p>Note however how in this case, no guarantees can be made that any XML inputs will result in valid JSON.
               Unless already in a form known in advance – <a
                  href="https://www.w3.org/TR/xpath-functions-31/#json-to-xml-mapping">the same vocabulary</a> defined
               by XPath –  XML inputs will typically result in errors, as no comprehensive, rules-bound cast can be
               defined across its variations. To alleviate this problem, exception handling logic in the form of an
               XProc <code>p:try/p:catch</code> can be found in the imported pipeline step (which performs the
               casting).</p>
            <p>Additionally, this variant has a fail-safe (look for <code>p:choose</code>) that prevents the production
               of JSON from files not named <code>*.xml</code> – strictly speaking, this is only a naming convention,
               but respecting it prevents unseen and unwanted name collisions. It does <i>not</i> defend against
               overwriting any other files that happen to be in place with the target name.</p>
         </section>
         <section>
            <h3><a href="../../../projects/oscal-convert/CONVERT-OSCAL-XML-DATA.xpl">CONVERT-OSCAL-XML-DATA</a></h3>
            <p>The requirement that any XML to be converted must already be JSON-ready by virtue of conforming to a
               JSON-descriptive vocabulary, is obviously an onerous one. To achieve a clean, complete and appropriate
               recasting and relabeling of data, depending on its intended semantics, the way those semantics are to be
               expressed in JSON must be fully defined.</p>
            <p>OSCAL solves this problem by defining its XML and JSON formats in parity, such that a complete
               bidirectional conversion can be guaranteed over data sets already schema-valid. The bidirectional
               conversions themselves can be performed implicitly or overtly by tools that read and write OSCAL, or they
               can be deployed as XSLT transformations, providing for the conversion to be performed by any XSLT 3.0
               engine.</p>
            <p>For XSLT 3.0, XProc has Saxon. The XSLTs in question can be acquired from an <a
                  href="https://github.com/usnistgov/OSCAL/releases/latest">OSCAL release</a>, as shown in the <a
                  href="../../../projects/oscal-convert/GRAB-RESOURCES.xpl">GRAB-RESOURCES</a> pipeline.</p>
            <p><a href="../../../projects/oscal-convert/CONVERT-OSCAL-XML-DATA.xpl">CONVERT-OSCAL-XML-DATA</a> applies
               one of these XSLTS to a set of given OSCAL XML files, valid to the catalog model, to produce JSON. It
               works on any XML file that has <code>catalog</code> as its root element, in the OSCAL namespace. It does
                  <i>not</i> provide for validation of the input against the schema: Instead, the garbage-in/garbage-out
               (GIGO) principle is respected. This means that some pipelines will run successfully while producing
               defective outputs, which must be discovered in the result (via formal validation and other checks). An
               XProc pipeline with a validation step preceding the conversion would show such errors earlier.</p>
            <p>The reverse pipeline is left as an exercise. Bring valid OSCAL JSON back into XML. Let us know if you
               have prototyped this and wish for someone to check your work! The task is not trivial because there are
               several ways to use the conversion stylesheet, any of which should work.</p>
         </section>
         <section>
            <h3><a href="../../../projects/oscal-convert/CONVERT-OSCAL-XML-FOLDER.xpl">CONVERT-OSCAL-XML-FOLDER</a></h3>
            <p>This pipeline performs the same conversion as <a
                  href="../../../projects/oscal-convert/CONVERT-OSCAL-XML-DATA.xpl">CONVERT-OSCAL-XML-DATA</a> with an
               important distinction: instead of designating its sources, it processes all XML files in a designated
               directory.</p>
         </section>
      </section>
      <section>
         <h2>Working concept: return trip</h2>
         <p>Note in this context that comparing the inputs and results of a round-trip conversion is an excellent way of
            determining, to some base level, the correctness and validity of your data set. While converting it twice
            cannot guarantee that anything in your data is <q>true</q>, if having converted XML to JSON and back again
            to XML, the result looks the same as the original, you can be sure that its <i>representation</i> is
            consistent.</p>
         <p>Here's an idea: a single pipeline that would accept either XML or JSON inputs, and produce either, or both,
            as outputs. Would that be useful? Should such an <q>OSCAL Army Knife</q> XProc also perform validation?</p>
      </section>
      <section>
         <h2>What is this XSLT?</h2>
         <p>Readers of the prior Lesson Unit on the <a href="../walkthrough/walkthrough_401_src.html" class="LessonUnit"
               > XSLT Factor</a>will already have seen XSLT.</p>
         <p>If your criticism of XProc so far is that it makes it look easy when it isn't, you have a point.</p>
         <p>Conversion from XML to JSON isn't free, assuming it works at all. The runtime might be effectively free, but
            developing it isn't.</p>
         <p>Here, the heavy lifting is done by the XSLT component - the Saxon engine invoked by the <code>p:xslt</code>
            step, applying logic defined in an XSLT stylesheet (aka <b>transformation</b>) stored elsewhere. It happens
            that a converter for OSCAL data is available in XSLT, so rather than having to confront this considerable
            problem ourselves, we drop in the solution we have at hand.</p>
         <p>In later units, more examples are shown of how using the XProc steps described, rudimentary data
            manipulations can be done using XProc by itself, without entailing the use of either XSLT or XQuery.</p>
         <p>At the same time, while pipelines are based on the idea of passing data through a series of processes, there
            are many cases where logic is sufficiently complex that it becomes essential to maintain – and test – that
            logic externally from the XProc. At what point it becomes more efficient to encapsulate logic separately
            (whether by XSLT, XQuery or other means), depends very much on the case.</p>
         <p>The <code>p:xslt</code> pipeline step in particular is so important for real-world uses of XProc that it is
            introduced early, to show such a black-box application. There is also an <a
               href="https://spec.xproc.org/3.0/steps/#c.xquery">XQuery</a> step – for many purposes, functionally
            equivalent. Among all these – XSLT, XQuery and XProc itself – there is useful redundancy and often more than
            one good way.</p>
         <p>XProc also makes a fine environment for testing XSLT developed or acquired to handle specific tasks – and it
            can support automated testing and test-driven development using <a
               href="https://github.com/xspec/xspec/wiki">XSpec</a>.</p>
         <p>Indeed XSLT and XQuery being, like XProc itself, declarative languages, it makes sense to factor out where
            we can while maintaining easy access and transparency for analysis and auditing purposes.</p>
      </section>
      <section>
         <h2>What could possibly go wrong?</h2>
         <p>Three things can happen when we run a pipeline:</p>
         <ul>
            <li>The pipeline can fail to run, typically terminating with an error message (or, unusually, failing to
               terminate)</li>
            <li>The pipeline can run successfully, but result in incorrect outputs given the inputs</li>
            <li>The pipeline can run successfully and correctly</li>
         </ul>
         <p>Among the range of possible errors, those that show up in your console with error messages are the easy
            ones. This will often be errors of <q>carelessness</q> (providing the wrong kind of input, etc.), easy to
            repair once found. Sometimes it is an input resource, not the pipeline, that must be corrected, or a
            different pipeline developed for the different input. If XML is expected but not provided, a conforming
            processor must emit an error. Correct it or plan on processing plain text.</p>
         <p>The second category is much harder. The most important concern when engineering a pipeline is to see to it
            that no data quality problems are introduced inadvertantly. Anomalous inputs might process <q>correctly</q>
            (for the input provided) but result in lost data or disordered results. Often this is obvious in testing,
            but not always. The key is defining and working within a scope of application (range of inputs) within which
               <q>correctness</q> can be specified, unambiguously and demonstrably, both with respect to the source
            data, and the processing requirement. Given such a specification, testing is possible. Without testing,
            things can be difficult or impossible to know, and evenif known, they can be difficult to demonstrate.</p>
         <p>While in comparison to syntax or configuration problems, data quality issues can be subtle, there is also
            good news: the very same tools we use to process inputs into outputs, can also be used to test and validate
            data to both applicable standards and local rules.</p>
         <p>Generally speaking, OSCAL maintains <em>validation parity</em> between its XML and JSON formats with respect
            to their schemas. That is to say, the XSD (XML schema) covers essentially the same set of rules for OSCAL
            XML data as the JSON Schema does for OSCAL JSON data, accounting for differences between the two notations,
            the data models and how information is mapped into them. A consequence of this is that valid OSCAL data,
            either XML or JSON, can reliably be converted to valid data in the other notation, while invalid data may
            come through with significant gaps, or not be converted at all.</p>
         <p>For this reason (as it applies to OSCAL) and related reasons on open systems (applying across the board, and
            not only to data conversions), the working principle in XML is often to define and formalize a model as
            early as possible – or identify and adopt a model already built – as a way to institute and enforce schema
            validation as a <i>prerequisite</i> and <i>primary requirement</i> for working with any data set. We do this
            by acquiring or writing and deploying schemas. To this end, XProc supports several kinds of schema
            validation including  <a href="https://spec.xproc.org/lastcall-2024-08/head/validation/#c.validate-with-dtd"
               >XML DTD (Document Type Definition)</a>, <a
               href="https://spec.xproc.org/lastcall-2024-08/head/validation/#c.validate-with-xml-schema">XSD (W3C
               Schema Definition language)</a>, <a
               href="https://spec.xproc.org/lastcall-2024-08/head/validation/#c.validate-with-relax-ng">RelaxNG (ISO
               ISO/IEC 19757-2)</a>, <a
               href="https://spec.xproc.org/lastcall-2024-08/head/validation/#c.validate-with-schematron">Schematron</a>
            and <a href="https://spec.xproc.org/lastcall-2024-08/head/validation/#c.validate-with-json-schema">JSON
               Schema</a>, making it straightforward to enforce this dependency at any point in a pipeline, whether
            applied to inputs or to pipeline results including interim results and pipeline outputs. Resource validation
            is described further in subsequent coverage including the next <a href="oscal-convert_102_src.html"
                   class="LessonUnit"><q>Maker</q> lesson unit</a>.</p>
         <section>
            <h3>The playing field is the Internet</h3>
            <p>File resources in XProc are designated and distinguished by URIs. Keep in mind that XProc in theory, and
               your XProc engine in practice, may read its inputs using whatever <a
                  href="https://www.iana.org/assignments/uri-schemes/uri-schemes.xhtml">URI schemes</a> it supports,
               while the schemes <code>file</code> and <code>http</code> (or <code>https</code>) are required for
               conformance, and work as they do on the Worldwide Web.</p>
            <p>Of course, permissions must be in place to read files from system locations, or save files to them. When
               authentication is configured or resources are openly available, using <code>http</code> to reach
               resources or sources can be a very convenient option.</p>
            <p>While this is important and powerful, it comes with complications. Internet access is not always a given,
               making such runtime dependencies fragile. XML systems that rely on URIs frequently also support one or
               another kind of URI indirection, such as <a
                  href="https://www.oasis-open.org/committees/entity/spec-2001-08-06.html">OASIS XML Catalogs</a>, to
               enable resource management, redirection and local caching of standard resources. For the XProc developer,
               this can be a silent source of bugs, hard to find and hard to duplicate and analyze. The <a
                  href="oscal-convert_102_src.html" class="LessonUnit">next lesson unit</a> describes some functions
               that can be used to provide the transparency needed.</p>
         </section>
      </section>
      <section>
         <h2>More catalogs needed!</h2>
         <p>As we go to press we have only one example OSCAL catalog to use for this exercise.</p>
         <p>Other valid OSCAL catalogs are produced from other projects in this repo, specifically <a
               href="../../../projects/CPRT-import/">CPRT import</a> and <a href="../../../projects/FM6-22-import/"
               >FM6-22-IMPORT</a>. Run the pipelines in those projects to produce more catalogs (in XML) useful as
            inputs here.</p>
      </section>
   </body>
</html>