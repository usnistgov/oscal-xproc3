<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <title>102: Hands on data conversions</title>
      <meta charset="utf-8" />
   </head>
   <body data-track="maker">
      <h1>102: Hands on data conversions</h1>
      <section>
         <h2>Goals</h2>
         <p>Learn how OSCAL data can be converted between JSON and XML formats, using XProc.</p>
         <p>Learn something about potential problems and limitations when doing this, and about how to detect, avoid,
            prevent or mitigate them.</p>
         <p>Work with XProc features designed for handling JSON data (XDM <b>map</b> objects that can be cast to
            XML).</p>
      </section>
      <section>
         <h2>Prerequisites</h2>
         <p>Run the pipelines described in <a href="https://github.com/usnistgov/oscal-xproc3/discussions/18">the 101
               Lesson</a></p>
      </section>
      <section>
         <h2>Resources</h2>
         <p>Same as the <a href="oscal-convert_101_src.html" class="LessonUnit">101 lesson</a>.</p>
      </section>
      <section>
         <h2>Some breaking and making</h2>
         <p>Every project you examine provides an opportunity to alter pipelines and see how they fail when not encoded
            correctly – when <q>broken</q>, any way we can think of breaking them. Then build good habits by repairing
            the damage. Experiment and observation bring learning.</p>
         <p>After reading this page and <a href="../../../projects/oscal-convert/readme.md">the project readme</a>, run
            the pipelines while performing some more disassembly / reassembly. Here are a few ideas (including a few you
            may have already done):</p>
         <ul>
            <li>Switch out the value of an <code>@href</code> on a <code>p:document</code> or <code>p:load</code> step.
               See what happens when the file it points to is not actually there.</li>
            <li>There is a difference between <code>p:input</code>, used to configure a pipeline in its prologue, and
                  <code>p:load</code>, a step that loads data. Ponder what these differences are. Try changing a
               pipeline that uses one into a pipeline that uses the other.</li>
            <li>Similarly, there is a difference between a <code>p:output</code> configuration for a pipeline, and a
                  <code>p:store</code> step executed by that pipeline. Consider this difference and how we might define
               a rule for when to prefer one or the other. How is the pipeline used - is it called directly, or intended
               for use as a step in other pipelines? How is it to be controlled at runtime?</li>
            <li>Try inserting <code>p:store</code> steps into a pipeline to capture intermediate results, that is, the
               output of any step before they are processed by the next step. Such steps can aid in debugging, among
               other uses.</li>
            <li><code>@message</code> attributes on steps provide messages for the runtime traceback. They are optional
               but this repo follows a rule that any <code>p:load</code> or <code>p:store</code> should be provided with
               a message. Why?</li>
            <li>A <code>p:identity</code> step passes its input unchanged to the next step. But can also be provided
               with a <code>@message</code>.</li>
         </ul>
         <p>After breaking anything, restore it to working order. Create modified copies of any pipelines for further
            analysis and discussion.</p>
         <ul>
            <li>Concept: copy and change one of the pipelines provided to acquire a software library or resource of your
               choice.</li>
         </ul>
      </section>
      <section>
         <h2>Value templates in attributes and text: { expr }</h2>
         <p>Practitioners of XQuery, XSLT and related technologies will recognize the curly-bracket characters (U+007B
            and U+007D) as indicators of <a href="https://www.w3.org/TR/xslt-10/#dt-attribute-value-template">attribute
               value templates</a>, <a href="https://www.w3.org/TR/xslt-30/#text-value-templates">text value
               templates</a>, or <a href="https://www.w3.org/TR/xquery-31/#id-enclosed-expr">enclosed expressions</a>.
            The expression within the braces is to be evaluated dynamically by the processor. This is one of the most
            useful convenience features in the language.</p>
         <p>These quickly become invisible. Upon seeing</p>
         <pre>&lt;p:identity message="Processing { $filename } at { current-date() }"/></pre>
         <p>the XProc developer understands:</p>
         <ul>
            <li>The date, in some form (try it and see) should be written into the message</li>
            <li>The variable reference <code>$filename</code> is defined somewhere, and here will expand to a
               string</li>
         </ul>
         <p>If you need to see actual curly braces, escape by doubling: <code>{{</code> for the single open and
               <code>}}</code> for the single close.</p>
         <p>Extra care must be taken with embedded XSLT and XQuery due to this feature, since their functioning will
            depend on correctly interpreting these within literal code. Yes, double escaping is sometimes necessary.
            (This can be tried with <a href="../../worksheets/NAMESPACE_worksheet.xpl">a worksheet XProc</a>.)</p>
         <p>Setting <code>expand-text</code> to <code>false</code> on an XProc element turns this behavior off: the
            braces become regular braces again. <a href="https://spec.xproc.org/3.0/xproc/#expand-text-attribute">The
               spec also describes</a> a <code>p:inline-expand-text</code> attribute that can be used in places (namely
            inside literal XML provided in your XProc using <code>p:inline</code>) where the regular expand-text has no
            effect. Either setting can be used inside elements already set, resulting in <q>toggling</q> behavior (it
            can be turned on and off), as any <code>expand-text</code> applies to override settings on its
            ancestors.</p>
      </section>
      <section>
         <h2>Designating an input at runtime by binding input ports</h2>
         <p>One potential problem with the pipelines we have looked at so far is that their inputs are hard-wired. While
            this is sometimes helpful, it should also be possible to apply a pipeline to an XML document (or other
            input) without having to designate the document inside the pipeline itself. The user or calling application
            should be able to say <q>run this pipeline, but this time with this input</q>.</p>
         <p>The input ports for a pipeline, specified using <code>p:input</code> within the prologue, provide for
            this.</p>
         <p>For example, the <a href="../../../projects/oscal-convert/CONVERT-OSCAL-XML-DATA.xpl"
               >CONVERT-OSCAL-XML-DATA</a> pipeline defines an input port:</p>
         <pre>&lt;p:input port="source" sequence="true">
    &lt;p:document href="data/catalog-model/xml/cat_catalog.xml"/>
&lt;/p:input></pre>
         <p>By default, this pipeline will pick up and process the data it finds at path
               <code>data/catalog-model/xml/cat_catalog.xml</code>, relative to the stylesheet. But any call to this
            pipeline, whether directly or as a step in another pipeline, can override this.</p>
         <p>The Morgana processor defines <a href="https://www.xml-project.com/manual/ch01.html#R_ch1_s1_2">a command
               syntax for binding inputs to ports</a>. It looks like this (when used with the script deployed with this
            repository):</p>
         <pre>$ ../xp3.sh <i>PIPELINE.xpl</i> -input:<i>portname=path/to/a-document.xml</i> -input:<i>portname=path/to/another-document.xml</i></pre>
         <p>Here, two different <code>-input</code> arguments are given for the same port. You can have as many as
            needed if the port, like this one, has <code>sequence="true"</code>, meaning any number of documents (from
            zero to many) can be bound to the port, and the pipeline will accommodate. When more than one port is
            defined, one (only) can be designated as <code>primary="true"</code>, meaning it will be provided implicitly
            when a port connection is required (by a step) but not given in the pipeline. Notice that the name of the
            port must also appear, as in <code>-input:portname</code>, since pipelines can have ports supporting
            sequences, but also as many input ports as it needs, named differently, for documents playing different
            roles in the pipeline. In place of <code>portname</code> here, a common name for a port (conventional when
            it is the pipeline's only or primary input) is <code>source</code>.</p>
         <section>
            <h3>Binding to input ports vs p:load steps</h3>
            <p>XProc offers two ways to acquire data from outside the pipeline: by using <code>p:load</code> or by
               binding inputs to an input port using <code>p:input/p:document</code>. These are somewhat different in
               operation - errors produced by <code>p:load</code> cannot be detected until the pipeline is run, whereas
               failures with <code>p:input</code> should be detected when the pipeline itself is loaded and compiled
               (i.e. during <em>static analysis</em>), and processors may be able to apply different kinds of exception
               handling, fallbacks or support for redirects. (As always you can try, test and determine for yourself.)
               Apart from this distinction the two approaches have similar effects – whether to use one or the other
               depends often on how you expect the pipeline to be used and distributed, not on whether it works.</p>
            <p>Although one distinction is that p:document appears on input ports, which can be overridden, this does
               not mean that p:document can't be essentially <q>private</q> to a pipeline or pipeline step. For example,
               if you wish to acquire more than a single document, without p:load, known in advance (i.e. the file names
               can be hard-coded), make a step like this:</p>
            <pre>&lt;p:identity>
  &lt;p:with-input>
    &lt;p:document href="..."/>
    &lt;p:document href="..."/>
    ...
  &lt;/p:with-input>
&lt;p:identity></pre>
            <p>This binds the documents to the input of an <b>identity</b> step (which supports a sequence), without
               exposing an input port in the main pipeline.</p>
            <p>A more dynamic approach is sometimes useful: first, acquire a list of file names, for example:</p>
            <pre>&lt;p:input port="source">
   &lt;p:inline>
      &lt;FILELIST>
         &lt;FILE>A&lt;/FILE>
         &lt;FILE>B&lt;/FILE>
      &lt;/FILELIST>
   &lt;/p:inline>
&lt;/p:input></pre>
            <p>Then in our subpipeline we use the compound step <code>p:for-each</code> to process each FILE element in
               the list:</p>
            <pre>&lt;p:for-each>
   &lt;p:with-input select="//FILE"/>
   &lt;p:load href="{ string(.) }"/>
&lt;/p:for-each>   </pre>
            <p>This has the effect of traversing the document given in line (the file list) and for each of its FILE
               elements, loading the document named as the FILE element's string value, that is <q>A</q>, <q>B</q> and
               so on. This is just as if A and B had been bound directly to the port. In either case, what we get is a
               sequence of XDM <em>document</em> objects, one for each of the resources parsed.</p>
            <p>One tradeoff is that the override mechanism will be different. We override the first approach by binding
               the pipeline's <code>source</code> port directly to whatever documents we want in place of A and B. We
               override the second approach by providing a different FILELIST document. Alternatively such a FILELIST
               can be referenced instead of included … <code>p:document href="the-filelist.xml</code>, providing us a
               resource that we can maintain separately.</p>
            <p>This makes the second approach especially appealing if the file list can be derived from some kind of
               metadata resource or, indeed, <code>p:directory-list</code>….</p>
            
         </section>
      </section>
      <section>
         <h2>Identity pipeline testbed</h2>
         <p>An identity or <q>near-identity</q> or modified-identity pipeline has its uses, including diagnostics. Since
            inputs and outputs are supposed to look the same, any changes they show between inputs and outputs can be
            revealing.</p>
         <p>They are also useful for testing features in your environment or setup, for example features for resource
            acquisition and disposition, that is, how you get data into your pipeline and then out again.</p>
         <p>Additionally, there are actually useful operations supported by a pipeline that presents its input unchanged
            with respect to its model. For example, it can be used to transcode a file from one encoding to another –
            changing nothing in the data, but rewriting it into a different character set. This is because with XProc,
            transcoding does not actually happen within the pipeline, but on its boundaries - when a file is read, or
            written (aka serialized). So internally, a pipeline set up to do this doesn't have any action to take.</p>
         <section>
            <h3>0.01 - what is a <q>document</q></h3>
            <p>Just about any kind of digital input can be an XProc document. Keeping things simple and regular, XProc's
               concept of document is broad enough to encompass XML, HTML, JSON and other kinds of inputs including
               plain text and binaries. <a href="oscal-convert_402_src.html" class="LessonUnit">Read more here</a>.</p>
         </section>
         <section>
            <h3>0.1 - loading documents known or discovered in advance</h3>
            <p>The XProc step <code>p:load</code> can be used to load the resource indicated into the pipeline.</p>
            <p>Watch out, since <code>p:load</code> with <code>href=""</code> – loading the resource at the location
               indicated by the empty string, <code>""</code> – will load the XProc file itself. This is conformant with
               rules for URL resolution.</p>
            <h3>0.2 - binding a document to an input port</h3>
            <h3>0.3 - loading documents discovered dynamically with <code>p:directory-list</code></h3>
            <h3>0.4 - saving results to the file system</h3>
            <h3>0.5 - exposing results on an output port</h3>
         </section>
      </section>
      <section>
         <h2>Probing error space - data conversions</h2>
         <p>Broadly speaking, problems encountered running these conversions fall into two categories, the distinction
            being simple, namely whether a bad outcome is due to an error in the processor and its logic, or in the data
            inputs provided. The term <q>error</q> here hides a great deal. So does <q>bad outcome</q>. One type of bad
            outcome takes the form of failures at runtime - the term <q>failure</q> again leaving questions open, while
            at the same time it seems fair to assume that not being able to conclude successfully, is bad. But other bad
            outcomes are not detectable at runtime. If inputs are bad (inconsistent with stated contracts such as data
            validation), processes can run <i>correctly</i> and deliver incorrect results: correctly representing
            inputs, in their incorrectness. Again, the term <i>correct</i> here is underspecified and underdefined,
            except in the case.</p>
         <p>For these and other reasons we sometimes prefer to call them <q>exceptions</q>, while at the same time we
            know many errors are not actually errors in the process but in the inputs. We need reliable ways to tell
            this difference. A library of reliable source examples -- a test suite – is one asset that helps a great
            deal. Even short of unit tests, however, a great deal can be discovered when working with <q>bad inputs</q>
            interactively.</p>
         <section>
            <h3>Converting broken XML or JSON</h3>
            <p>Create a syntactically-invalid (not <b>well-formed</b>) XML or JSON document - or rather (more
               correctly), a text document that might have been XML or JSON but for some incidental problem.</p>
            <p><i>Try using this as input</i> to any XProc. Note how the processor delivers error messages bringing
               attention to problems it discovers.</p>
         </section>
         <section>
            <h3>Converting not-OSCAL</h3>
            <p>XML practitioners understand how XML can be well-formed and therefore legible for processing, without
               being a valid instance of a specific markup vocabulary. You can have XML, for example, without having
               OSCAL.</p>
            <p>When providing XML that is not OSCAL to a process that expects OSCAL inputs, you should properly see
               either errors (exceptions), or bad results (outputs missing or wrongly expressed) or both. <i>Experiment
                  and see!</i></p>
            <p>Detection of bad results is an important capability - why we have validation against external constraint
               sets such as schemas. A later unit will cover this – meanwhile, inquiries on the topic are welcome.</p>
         </section>
         <section>
            <h3>Converting broken OSCAL</h3>
            <p>The same thing applies to attempting to process inputs when OSCAL is expected, yet the data sources fail
               to meet requirements in some important respect, sometimes even a subtle requirement, depending on the
               case. The more fundamental problem here is the definition of <q>correct</q> versus <q>broken</q>.</p>
            <p>We begin generally with the stipulation that by <q>OSCAL</q> what we mean is, any XML (or JSON or YAML)
               instance conformant to an OSCAL schema, and thereby defined in such a manner as to enable their
               convertibility. The reasoning is thus somewhat circular. If we can convert it successfully, we can claim
               to know it as OSCAL (by virtue of the knowledge we demonstrate in the conversion). If we know it to be
               OSCAL by virtue of schema validation, we have assurances also regarding its convertibility.</p>
            <p>This is because with respect to these model-based conversions, the OSCAL project also offers tools that
               can convert any schema-valid OSCAL XML into equivalent schema-valid JSON, while doing the same the other
               way, making OSCAL XML from OSCAL JSON. In either case, schema validation is invaluable for defining the
               boundaries of the conversion itself. Data that is not schema-valid, it is reasoned, cannot be qualified
               or described at all, so no straightforward mapping from arbitrary inputs can be specified. But a mapping
               can be specified for inputs that are known, namely OSCAL inputs. The converter respects the validation
               rule not by enforcing it directly, but rather by depending on it.</p>
            <p>Fortunately, by means of Schematron and transformations, XProc is an excellent tool not only for altering
               data sets, but also for detecting variances, either in inputs or its results, from any specifications
               that can be expressed in XPath. These capabilities – detection and amelioration – can be used together,
               and separately. When a pipeline cannot guarantee correct outputs, it can at least provide feedback.</p>
            <p>Altering XML to <q>break</q> it in various subtle ways is likely to happen by accident. Get used to the
               feeling by <i>making it happen</i> on purpose.</p>
         </section>
      </section>
      <section>
         <h2>XProc diagnostic how-to</h2>
         <section>
            <h3>Emitting runtime messages</h3>
         </section>
         <section>
            <h3>Saving out interim results</h3>
            <p><code>p:store</code></p>
         </section>
      </section>
      <section>
         <h2>Validate early and often</h2>
      </section>
   </body>
</html>