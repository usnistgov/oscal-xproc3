<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <title>102: Hands on data conversions</title>
      <meta charset="utf-8" />
   </head>
   <body data-track="maker">
      <h1>102: Hands on data conversions</h1>
      <section>
         <h2>Goals</h2>
         <p>Learn how OSCAL data can be converted between JSON and XML formats, using XProc.</p>
         <p>Learn about potential problems and limitations when doing this, and about how to detect, avoid, prevent or
            mitigate them.</p>
         <p>Learn something about XProc features designed for handling JSON data (XDM <b>map</b> objects that can be
            cast to XML).</p>
      </section>
      <section>
         <h2>Prerequisites</h2>
         <p>Run the pipelines described in <a href="oscal-convert_101_src.html" class="LessonUnit">the 101 Lesson
               Unit</a> in this topic.</p>
      </section>
      <section>
         <h2>Resources</h2>
         <p>Same as the <a href="oscal-convert_101_src.html" class="LessonUnit">101 lesson</a>.</p>
      </section>
      <section>
         <h2>Some breaking and making</h2>
         <p>Every project you examine provides an opportunity to alter pipelines and see how they fail when not encoded
            correctly – when <q>broken</q>, any way we can think of breaking them. Then build good habits by repairing
            the damage. Experiment and observation bring learning.</p>
         <p>After reading this page and <a href="../../../projects/oscal-convert/readme.md">the project readme</a>, run
            the pipelines while performing some more disassembly / reassembly. Here are a few ideas:</p>
         <ul>
            <li>Switch out the value of an <code>@href</code> on a <code>p:document</code> or <code>p:load</code> step.
               See what happens when the file it points to is not actually there.</li>
            <li>There is a difference between <code>p:input</code>, used to configure a pipeline in its prologue, and
                  <code>p:load</code>, a step that loads data. Ponder what these differences are. Try changing a
               pipeline that uses one into a pipeline that uses the other.</li>
            <li>Similarly, there is a difference between a <code>p:output</code> configuration for a pipeline, and a
                  <code>p:store</code> step executed by that pipeline. Consider this difference and how we might define
               a rule for when to prefer one or the other. How is the pipeline used – is it called directly, or intended
               for use as a step in other pipelines? How is it to be controlled at runtime?</li>
            <li>Try inserting <code>p:store</code> steps into a pipeline to capture intermediate results, that is, the
               output of any step before they are processed by the next step. Such steps can aid in debugging, among
               other uses.</li>
            <li><code>@message</code> attributes on steps provide messages for the runtime traceback. They are optional
               but this repo follows a rule that any <code>p:load</code> or <code>p:store</code> should be provided with
               a message. Why?</li>
            <li>A <code>p:identity</code> step passes its input unchanged to the next step. It can also be provided with
               a <code>@message</code>. The two commonest uses of <code>p:identity</code> are probably to provide for a
                  <q>no-op</q> option, for example within a conditional or try/catch – and to provide runtime messages
               to the console.</li>
         </ul>
         <p>After breaking anything, restore it to working order. Create modified copies of any pipelines for further
            analysis and discussion.</p>
         <ul>
            <li>Concept: copy and change one of the pipelines provided to acquire a software library or resource of your
               choice.</li>
         </ul>
      </section>
      <section>
         <h2>Value templates in attributes and text: { XPath-expr }</h2>
         <p>Practitioners of XQuery, XSLT and related technologies will recognize the curly-bracket characters (U+007B
            and U+007D) as indicators of <a href="https://www.w3.org/TR/xslt-10/#dt-attribute-value-template">attribute
               value templates</a>, <a href="https://www.w3.org/TR/xslt-30/#text-value-templates">text value
               templates</a>, or <a href="https://www.w3.org/TR/xquery-31/#id-enclosed-expr">enclosed expressions</a>.
            The expression within the brackets is to be evaluated dynamically by the processor. This is one of the most
            useful convenience features in the language.</p>
         <p><a href="https://spec.xproc.org/3.0/xproc/#value-templates">This syntax</a> is concise, but expressive. Upon
            seeing:</p>
         <pre>&lt;p:identity message="Processing { $filename } at { current-date() }"/></pre>
         <p>the XProc developer understands:</p>
         <ul>
            <li>The date, in some form should be written into the message. (Try it and see.) The XPath function <a
                  href="https://www.w3.org/TR/xpath-functions-31/#func-format-date">format-date</a> can also be used if
               we want a different format: for example, <code>current-date() => format-date('[D] [MNn]
               [Y]')</code>.</li>
            <li>The variable reference <code>$filename</code> is defined somewhere, and here will expand to a string
               value due to the operation of the (attribute value) template.</li>
         </ul>
         <p>If you need to see actual curly brackets, escape by doubling: <code>{{</code> for the single open and
               <code>}}</code> for the single close.</p>
         <p>One complication arises: because XSLT and XQuery support similar syntax, clashes can occur, since their
            functioning will depend on correctly interpreting the syntax within literal code. Yes, this means double
            escaping is sometimes necessary. (This can be tried with <a href="../../worksheets/NAMESPACE_worksheet.xpl"
               >a worksheet XProc</a>.)</p>
         <p>Alternatively, setting <code>expand-text</code> to <code>false</code> on an XProc element turns this
            behavior off: the brackets become regular brackets again. <a
               href="https://spec.xproc.org/3.0/xproc/#expand-text-attribute">The spec also describes</a> an attribute
               <code>p:inline-expand-text</code> that can be used in places where the regular <code>expand-text</code>
            would interfere with a functional requirement (namely the representation of literal XML provided in your
            XProc using <code>p:inline</code>). Either of these settings can be used inside elements already set,
            resulting in <q>toggling</q> behavior (it can be turned on and off), as any <code>expand-text</code>, by
            applying to descendants, overrides settings on its ancestors.</p>
         <p>For the most part it is enough to know that the <code>expand-text</code> setting is <q>on</q>
               (<code>true</code>) by default, but it can be turned off (<code>false</code>) – and (for handling edge
            cases) back on, lower down in the hierarchy.</p>
      </section>
      <section>
         <h2>Designating inputs</h2>
         <p>One potential problem with the pipelines we have looked at so far is that their inputs are hard-wired. While
            this is sometimes helpful, it should also be possible to apply a pipeline to an XML document (or other
            input) without having to designate the document inside the pipeline itself. The user or calling application
            should be able to say <q>run this pipeline, but this time with this input</q>.</p>
         <p>The input ports for a pipeline, specified using <code>p:input</code> within the prologue, provide for
            this.</p>
         <p>For example, the <a href="../../../projects/oscal-convert/CONVERT-OSCAL-XML-DATA.xpl"
               >CONVERT-OSCAL-XML-DATA</a> pipeline defines an input port:</p>
         <pre>&lt;p:input port="source" sequence="true">
    &lt;p:document href="data/catalog-model/xml/cat_catalog.xml"/>
&lt;/p:input></pre>
         <p>By default, this pipeline will pick up and process the data it finds at path
               <code>data/catalog-model/xml/cat_catalog.xml</code>, relative to the pipeline instance (XProc file). But
            any call to this pipeline, whether directly or as a step in another pipeline, can override this.</p>
         <p>The Morgana processor defines <a href="https://www.xml-project.com/manual/ch01.html#R_ch1_s1_2">a command
               syntax for binding inputs to ports</a>. It looks like this (when used with the script deployed with this
            repository):</p>
         <pre>$ ../xp3.sh <i>PIPELINE.xpl</i> -input:<i>portname=path/to/a-document.xml</i> -input:<i>portname=path/to/another-document.xml</i></pre>
         <p>Here, two different <code>-input</code> arguments are given for the same port. You can have as many as
            needed if the port, like this one, has <code>sequence="true"</code>, meaning any number of documents (zero,
            one or more) can be bound to the port, and the pipeline will accommodate. When more than one port is defined
            for a pipeline, one (only) can be designated as <code>primary="true"</code>, allowing it to be provided
            implicitly when a port connection is required (by a step) but not given in the pipeline. Notice that the
            name of the port must also appear in the command argument, as in <code>-input:portname</code>, since while
            pipelines can have ports supporting sequences, they will also have different ports, named differently, for
            documents playing different roles in the pipeline.</p>
         <p>In place of <code>portname</code> here, a common name for a port (conventional when it is the pipeline's
            only or primary input) is <code>source</code>. But you can also expect to see ports (especially secondary
            ports) with names like <code>schema</code>, <code>stylesheet</code> and <code>insertion</code>: port names
            that offer hints as to what the step does.</p>
         <p>A port designated with <code>sequence="true"</code> can be empty (no documents at all) and a process will
            run. But by default a single document is both expected and required.</p>
         <p>Among other things, this means that a pipeline that has <code>&lt;p:input name="x" primary="true"/></code>,
            since it is not a sequence but also has no document, cannot be run unless a (single) document for the
               <code>x</code> port (as it is named here) is provided when it is invoked.</p>
         <section>
            <h3>Lightening the <code>p:load</code></h3>
            <p>As an alternative to binding inputs to using <code>p:input/p:document</code> (on a pipeline definition)
               or <code>p:with-input</code> (on a step invocation), XProc offers another way to acquire data from
               outside the pipeline: by using a <code>p:load</code> step. This is somewhat different in operation: as it
               is a step in itself, errors produced by <code>p:load</code> cannot be detected until the pipeline is run,
               whereas failures with <code>p:input</code> should be detected when the pipeline itself is loaded and
               compiled (i.e. during <em>static analysis</em>), and processors may be able to apply different kinds of
               exception handling, fallbacks or support for redirects. (As always you can try, test and determine for
               yourself.) Apart from this distinction the two approaches have similar effects – whether to use one or
               the other depends often on how you expect the pipeline to be used, distributed, and maintained, since
               either can work in operation.</p>
            <p>Although one distinction is that <code>p:document</code> appears on input ports, which can be overridden
               (or rather, set dynamically), this does not mean that <code>p:document</code> cannott be essentially
                  <q>private</q> to a pipeline or pipeline step. For example, if you wish to acquire, without
                  <code>p:load</code>, more than a single document known in advance (i.e. the file names can be
               hard-coded), provide your step (<code>p:identity</code> in this case) with inputs like so:</p>
            <pre>&lt;p:identity>
  &lt;p:with-input>
    &lt;p:document href="..."/>
    &lt;p:document href="..."/>
    ...
  &lt;/p:with-input>
&lt;p:identity></pre>
            <p>This binds the documents to the input of the step (as <code>p:identity</code> supports a sequence, more
               than one is fine), without exposing an input port in the main pipeline.</p>
            <p>Combining the approaches permits another useful capability: first, acquire a list of file names, for
               example (here using <code>p:input/p:inline)</code>:</p>
            <pre>&lt;p:input port="source">
   &lt;p:inline>
      &lt;FILELIST>
         &lt;FILE>A&lt;/FILE>
         &lt;FILE>B&lt;/FILE>
      &lt;/FILELIST>
   &lt;/p:inline>
&lt;/p:input></pre>
            <p>Then in our subpipeline we use the compound step <code>p:for-each</code> to process each FILE element in
               the list:</p>
            <pre>&lt;p:for-each>
   &lt;p:with-input select="//FILE"/>
   &lt;p:load href="{ string(.) }"/>
&lt;/p:for-each>   </pre>
            <p>This has the effect of traversing the document given in line (the file list) and for each of its FILE
               elements, loading the document named as the FILE element's string value, that is <q>A</q>, <q>B</q> and
               so on. This is just as if A and B had been bound directly to the port. In either case, what we get is a
               sequence of XDM <em>document</em> objects, one for each of the resources parsed.</p>
            <p>One tradeoff is that the override mechanism will be different. We override the first approach by binding
               the pipeline's <code>source</code> port directly to whatever documents we want in place of A and B. We
               override the second approach by providing a different FILELIST document. Alternatively such a FILELIST
               can be referenced instead of included … <code>p:document href="the-filelist.xml</code>, providing us a
               resource that we can maintain separately.</p>
            <p>This makes the second approach especially appealing if the file list can be derived from some kind of
               metadata resource or, indeed, <code>p:directory-list</code>….</p>
            
         </section>
      </section>
      <section>
         <h2>Warning: do you know where your source files are?</h2>
         <p>As noted in the <a href="oscal-convert_101_src.html" class="LessonUnit">101 Lesson Unit</a>, one of the
            advantages of using URIs, over and above the Internet itself, is that systems can support URI redirection
            when appropriate. This will ordinarily be in order to provide local (cached) copies of standard resources,
            thereby mitigating the need for copying files over the Internet. While this is a powerful and useful feature
            – arguably essential for systems at scale – it can present problems for transparency and debugging if the
            resource obtained by reference to a URI is not the same as the developer (or <q>contract</q>) expects.</p>
         <p>A similar problem results from variations in URI syntax, both due to syntax itself and due to the fact that
            URIs can be relative file paths, so <code>file.xml</code> and <code>../file.xml</code> could be the same
            file, or not, depending on the context of evaluation.</p>
         <p>To help avoid or manage problems resulting from this (i.e., from features as bugs), XPath and XProc offer
            some useful functions:</p>
         <ul>
            <li>XPath <a href="https://www.w3.org/TR/xpath-functions-31/#func-resolve-uri">resolve-uri()</a> can be used
               to expand a relative URI into an absolute URI</li>
            <li>XProc <a href="https://spec.xproc.org/3.0/xproc/#f.urify">p:urify</a> will normalize URIs and rewrite
               file system paths as URIs – very useful.</li>
            <li>In XProc 3.1, a new function <a href="https://spec.xproc.org/lastcall-2024-08/head/xproc/#f.lookup-uri"
                  >p:lookup-uri</a> can query the processor's URI resolver regarding a URI, without actually retrieving
               its resource. This makes available to the developer what address is actually to be used when a URI is
               followed – detecting any redirection – and permits defensive code to be written when appropriate.</li>
         </ul>
      </section>
      <section>
         <h2>Probing error space – data conversions</h2>
         <p>Broadly speaking, problems encountered running these conversions (or indeed, transformations in general)
            fall into two categories, the distinction being simple, namely whether a bad outcome is due to an error in
            the processor and its logic, or in the data inputs provided. The term <q>error</q> here hides a great deal.
            So does <q>bad outcome</q>. One type of bad outcome takes the form of failures at runtime – the term
               <q>failure</q> again leaving questions open, while at the same time it seems fair to assume that not
            being able to conclude successfully is a bad thing. But other bad outcomes are not detectable at runtime. If
            inputs are bad (inconsistent with stated contracts such as data validation), processes can run
               <i>correctly</i> and deliver incorrect results: correctly representing inputs, in their incorrectness.
            Again, the term <i>correct</i> here is underspecified and underdefined, except in the case.</p>
         <p>For these and other reasons we sometimes prefer to call them <q>exceptions</q>, while at the same time we
            know many errors are not actually errors in the process but in the inputs. We need reliable ways to tell
            this difference. A library of reliable source examples -- a test suite – is one asset that helps a great
            deal. Even short of unit tests, however, a great deal can be discovered when working with <q>bad inputs</q>
            interactively. This knowledge is especially valuable once we are dealing with examples that are only
               <q>normally bad</q>.</p>
         <p>Some ideas on how to do this appear below.</p>
         <section>
            <h3>Converting broken XML or JSON</h3>
            <p>Create a syntactically-invalid (not <b>well-formed</b>) XML or JSON document - or rather (more
               correctly), a text document that might have been XML or JSON but for some incidental problem.</p>
            <p><i>Try using this as input</i> to any XProc. Note how the processor delivers error messages bringing
               attention to problems it discovers.</p>
         </section>
         <section>
            <h3>Converting not-OSCAL</h3>
            <p>XML practitioners understand how XML can be well-formed and therefore legible for processing, without
               being a valid instance of a specific markup vocabulary. You can have XML, for example, without having
               OSCAL. This was discussed in <a href="oscal-convert_101_src.html" class="LessonUnit">the previous lesson
                  unit</a>.</p>
            <p>But a hands-on appreciation, through experience, of how this actually looks, is better than a merely
               intellectual understanding of why it must be.</p>
            <p>When providing XML that is not OSCAL to a process that expects OSCAL inputs, you should properly see
               either errors (exceptions), or bad results (outputs missing or wrongly expressed) or both. <i>A tutorial
                  is the perfect opportunity to experiment and see.</i></p>
            <p>For example, try using the OSCAL XML-to-JSON pipeline on an XProc document (which is XML, but not
               OSCAL).</p>
            <p>The interesting thing here is how permissive XProc is, unless we code it to be jealous. Detection of bad
               results is an important capability, which is why we also need to be able to <em>validate</em> data
               against external constraint sets such as schemas, also covered in more detail later.</p>
         </section>
         <section>
            <h3>Converting broken OSCAL</h3>
            <p>The same thing applies to attempting to process inputs when OSCAL is expected, yet the data sources fail
               to meet requirements in some important respect, sometimes even a subtle requirement, depending on the
               case. The more fundamental problem here is the definition of <q>correct</q> versus <q>broken</q>.</p>
            <p>We begin generally with the stipulation that by <q>OSCAL</q> what we mean is, any XML (or JSON or YAML)
               instance conformant to an OSCAL schema, and thereby defined in such a manner as to enable their
               convertibility. The reasoning is thus somewhat circular. If we can convert it successfully, we have a
               basis to claim it is OSCAL, by virtue of its <i>evident</i> conformance to OSCAL models in operation. If
               we know it to be OSCAL by virtue of schema validation, we have assurances also regarding its
               convertibility.</p>
            <p>In contrast, data that is not schema-valid (as can be reasoned) cannot be <i>confidently</i> and
                  <i>completely</i> qualified or described at all, so only very simple (<q>global</q>, generic or
                  <q>wildcard</q>) mappings from arbitrary inputs can be specified. But a mapping can be specified for
               inputs that are known, such as OSCAL inputs. An OSCAL converter respects the validation rules not by
               enforcing them directly, but rather by depending on the consistency they describe and constrain.</p>
            <p>Fortunately, by means of Schematron and transformations, XProc is an excellent tool not only for altering
               data sets, but also for imposing such validation rules, by detecting variances, either in inputs or its
               results. XPath, the query language, becomes key. With XPath to identify features (both good and bad), and
               XProc for modifications, these capabilities – detection and amelioration – can be used together, and
               separately. When a pipeline cannot guarantee correct outputs, it can at least provide feedback.</p>
            <p>Depending on the application and data sources, XML that is <q>broken</q> in various subtle ways is more
               or less inevitable. See what it looks like by making this happen on purpose.</p>
         </section>
      </section>
      <section>
         <h2>XProc diagnostic how-to</h2>
         <p>These methods are noted above, but they are so important they should not be skipped.</p>
         <section>
            <h3>Emitting runtime messages</h3>
            <p>Most XProc steps support a <code>message</code> attribute for designating a message to be emitted to the
               console or log. As shown, these also support Attribute Value Syntax for dynamic evaluation of XPath.</p>
            <p>For example, again using <code>p:identity</code>:</p>
            <pre>&lt;p:identity message="Processing { p:document-property(.,'base-uri') } with content-type { p:document-property(.,'content-type') }"/></pre>
            <p>This step does not change the document, but reports its current Base URI and content-type at that point
               in the pipeline.</p>
            <p>This can be useful information since both those properties can (and should) change based on your
               pipeline's operations.</p>
         </section>
         <section>
            <h3>Saving out interim results</h3>
            <p>Learn to use the <code>p:store</code> step, if only because it is so useful for saving interim pipeline
               results to a place where they can be inspected.</p>
            <p><a href="../../../projects/FM6-22-import/PRODUCE_FM6-22-chapter4.xpl">Produce-FM6-22-chapter4</a> is a
               demonstration pipeline in this repo with a switch at the top level, in the form of an option named
                  <code>writing-all</code>. When set to <code>true()</code>, it has the effect of activating a set of
                  <code>p:store</code> steps within the pipeline using the XProc <a
                  href="https://spec.xproc.org/3.0/xproc/#use-when">use-when feature</a> feature, to write intermediate
               results. The resulting set of files is written into a <code>temp</code> directory to keep them separate
               from final results: they show the changes being made over the input data set, at useful points for
               tracing the pipeline's progress.</p>
         </section>
      </section>
      <section>
         <h2>Validate early and often</h2>
         <p>One way to manage the problem of ensuring quality is to validate the inputs before processing, either as a
            dependent (prerequisite) process, or built into a pipeline. This enables a useful separation between
            problems resulting from bad inputs, and problems within the pipeline. Whatever you want to do with invalid
            inputs, including skipping or ignoring them, producing warnings or runtime exceptions, or even making
            corrections when possible and practical – all this can be defined in a pipeline much like anything else.</p>
         <p>Keep in mind that since XProc brings support for multiple schema languages plus XPath, <q>validation</q>
            could mean almost anything. This must be determined for the case.</p>
         <p>In the <a href="../../../projects/oscal-publish/publish-oscal-catalog.xpl">publishing demonstration project
               folder</a> is an XProc that valides XML against an OSCAL schema, before running steps to  convert it to
            HTML, for display in a browser. The same could be done for an XProc that converts OSCAL data into JSON --
            since OSCAL has both XSD for XML, and JSON Schema for JSON, this could be done before the conversion, after,
            or both.</p>
         <p>Two projects in this repository (at time of writing) deal extensively with validation: <a
               href="../../../projects/oscal-validate/">oscal-validate</a> and <a
               href="../../../projects/schema-field-tests/">schema-field-tests</a>.</p>
      </section>
   </body>
</html>